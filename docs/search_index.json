[["index.html", "EFI Task Views Introduction", " EFI Task Views Ecological Forecasting Initiative Last modified: 2022-09-16 Introduction Caption for the picture. The EFI Methods and Cyberinfrastructure EFI Working Groups are compiling common tasks in ecological forecasting and methods and tools to help with tasks commonly used in ecological forecasting for the following categories. Reproducible Forecasting Workflows Modeling &amp; Statistical resources, including Uncertainty Quantification &amp; Propagation Data Ingest, Cleaning, and Management Visualization, Decision Support, and User Interfaces. These Resources and tools listed in the four categories of tasks are meant to be living documents. This list is not meant to be a comprehensive overview of all possible resources, as there are some tasks where there are hundreds of different tools available. Instead we focus on commonly used tools. However, if there are often used tools and resources we are missing, we welcome input from anyone — suggestions can be shared as Issues to the eco4cast/taskview GitHub repository. "],["reproducible-forecasting-workflows.html", "1 Reproducible Forecasting Workflows 1.1 Overview 1.2 Scripted Analysis 1.3 Project Structure 1.4 Version Control 1.5 Literate Programming 1.6 Workflows and Dependency Management 1.7 Unit Testing 1.8 Continuous Integration and Automation 1.9 Containerization 1.10 Metadata 1.11 Data and Code Release", " 1 Reproducible Forecasting Workflows updated: 2021-07-27 1.1 Overview Curators: Jake Zwart1, Alexey Shiklomanov2, Kenton McHenry3, Daniel S. Katz3, Rob Kooper3, Carl Boettiger4, Bryce Mecum5, Mike Dietze6, Quinn Thomas7 1USGS, 2NASA, 3National Center for Supercomputing Applications at the University of Illinois Urbana-Champaign, 4University of California, Berkeley, 5National Center for Ecological Analysis and Synthesis, 6Boston University, 7Virginia Tech Reproducibility§ of scientific output is of utmost importance because it builds trust between stakeholders and scientists, increases scientific transparency, and often makes it easier to build upon methods and/or add new data to an analysis. Reproducibility is particularly critical when forecasting, because the same analyses need to be repeated again and again as new data become available, often in an automated workflow. Furthermore, reproducible workflows are essential to benchmark forecasting skill, improve ecological models, and analyze trends in ecological systems over time. All forecasting projects, from single location forecasts disseminated to only a few stakeholders to daily-updating continental-scale forecasts for public consumption, benefit from using tools and techniques that enable reproducible workflows. However, the project size often dictates which tools are most appropriate, and here we give a brief introductory overview of some of the tools available to help make your ecological forecast reproducible. There are many more tools available than what we describe here, and we primarily focus on open-source software to facilitate reproducibility independent of software access. § Reproducibility is the degree of agreement among results generated by at least two independent groups using the same suite of methods. Many of the tools highlighted here facilitate repeatability, which is a measure of agreement among results from a single task that is performed by the same person or tool. Repeatability is necessary for reproducible forecasting workflows. 1.2 Scripted Analysis Forecasts produced without using scripted analyses are often inefficient and prone to non-reproducible output. Therefore, it is best to perform data ingest and cleaning, modeling, data assimilation, and forecast visualization using a scripted computing language that perform tasks automatically once properly configured. 1.2.1 Tools for scripted analysis Interpreted languages allow the user to execute commands line-by-line, interactively, in real time. This makes debugging and exploratory analysis much easier, and significantly reduces programmer time for performing analyses. Analyses using interpreted languages are also usually easier to reproduce because of fewer installation/configuration steps (and more standardized, centralized installation mechanisms). This convenience generally comes at the expense of computational speed, but many times the tradeoff is worth it. R – Originally developed for statistical computing, and still primarily used for data science and other scientific computing tasks. Many important data science tools, including statistical distributions, plotting, and tabular data analysis, are included in the core language. Tens of thousands of add-on packages for just about any task imaginable exist. Python – General purpose programming language with a much more limited set of core features than R. Many data-science features are accessible through add-on packages and are curated through repositories such as PyPi, Anaconda, and Enthought. Julia – Very recent language. Claims to combine the ease-of-use of interpreted languages like R and Python with the performance of compiled languages like C and Fortran. Specifically relevant to forecasting and uncertainty propagation, Julia has extremely powerful probabilistic programming tools (e.g. Turing for Bayesian inference, Flux for machine learning). Compiled languages generally perform computationally intensive tasks much faster (up to 80x or more) than interpreted languages. However, their syntax is generally stricter / less forgiving, and analyses have to be written as complete programs that are compiled in operating system-specific (and even machine-specific) ways. In general, these languages should be avoided in favor of easier-to-use interpreted languages unless you are addressing specific computational bottlenecks. Note that all of the interpreted languages above provide ways to call specific functions/subroutines written in these compiled languages, so you have the option to only use these routines for specific, computationally-limiting steps in your analysis. Commonly used compiled languages include: C C++ Fortran A good standard is to develop an analysis using an interpreted language first and assess if it is fast enough for your needs. If it is fast enough, then you are done! If not, do some basic profiling to identify performance bottlenecks. See if there are existing tools or techniques in the language you are using that can help address the bottlenecks. Only fall back on compiled languages if you’ve reasonably exhausted possibilities using your current language. List of programming languages 1.3 Project Structure Organized project structures help the scientist and collaborators navigate the workflow of the ecological forecasting project from data input to dissemination of results. Subfolders should be used to break up the project into conceptually distinct steps of the forecasts and sequentially numbering of scripts and subfolders helps with readability, for example, “10_data”, “20_clean”, “40_forecast”, “60_visualize”, “95_report” (see more detailed example below). The number prefixes should represent a conceptual workflow for each forecasting project, and subdirectories within each phase of the project should describe the inputs, outputs, and functions for each step. Generally, unnumbered directories should contain supporting files that apply to the overall project, for example a configuration file that is used in multiple phases of the forecasting project. Example of organized folder and file structure for a forecasting project. 1.3.1 Tools for organized project structures Project-oriented workflows are self-contained workflows enabling reproducibility and navigability when used in conjunction with organized project structures for ecological forecasting projects. Ideally a collaborator should be able to run the entire project without changing any code or files (e.g. file paths should be workstation-independent). R and Python both have options for enabling self-contained workflows in their coding environments. R – RStudio projects – R projects allow for analyses to be contained in a single working directory that can be given to a collaborator and run without changing file directory paths. Python – Spyder projects – Python projects also allow for self-contained analyses and integration with Git version control (see Version Control below). 1.4 Version Control Version control is the process of managing changes in code and workflows and improves transparency in the code development process and facilitates open science and reproducibility. Code versioning also enables experimentation and development of code on different “branches” while retaining canonical files that can be used in operations, for example. Modern version control systems make it easy to create and switch between branches within a code base, encouraging developers to experiment without potentially breaking changes without worrying about losing stable code. This is especially useful for forecasting projects that need to make forecasts at regular schedules (e.g. daily), while researchers can also make alterations to the code base on experimental branches. Finally, version control facilitates collaboration by formalizing the process for introducing changes and keeping a record of who introduced which changes, when, and why. Additionally, version control allows contributions in an open way from even unknown contributors with the opportunity for the main authors control which contributions are accepted. Software development is a trillion dollar industry and it is well worth the time learning the basics of industry standard tools like version control, rather than relying on ad hoc and error prone approaches such as file naming (e.g. script.v2.R, python_script_final_FINAL.py), Dropbox/Google Drive, or emailing files to collaborators. 1.4.1 Tools for version control The distributed model of version control is where developers of code work from local repositories which are linked to a central repository. This enables automatic branching and merging, improves the ability to work offline, and doesn’t rely on a single repository for backup. Git is the most popular open-source version control system among ecologists and also professional software developers. The popularity enables contributions from many collaborators since potential contributors will likely be used to using Git and web interfaces like GitHub. You can practice using Git for version control with some simple tutorials. Rstudio has integrated support for GitHub Spyder projects (Python) and Integrated Develop Environments such as PyCharm and Visual Studio Code have integration with Git. GitLab also uses Git, and similar to GitHub allows for issue tracking and various other project management tools, and GitLab provides more options for collaborator authentication. Example of version control workflow using Git. Figure from here. List of other version control programs 1.5 Literate Programming Traditionally, scientific writing and coding are separate activities—for example, a researcher who wants to use code to generate a figure for her paper will have the code for generating that figure in one file and the document itself in another. This is a challenge for reproducibility and provenance tracking because both criteria have to be maintained for multiple files simultaneously. “Literate programming” provides an alternative approach, whereby code and text are interleaved within a single file; these files can be processed by special literate programming software to produce documents with the output of the code (e.g. figures, tables, and summary statistics) automatically interspersed with the document’s body text. This approach has several advantages. For one, the code output of a literate programming document is by definition guaranteed to be consistent with the code in the document’s source. At the same time, literate programming can make it easier to develop analyses by reducing the separation between writing and coding; for instance, interactive literate programming software can be used to keep “digital lab notebooks” where analyses are developed and described in the same file. In the context of ecological forecasting, literate programming techniques can be particularly useful for writing forecast software documentation, and can even be used for creating automatically-updating documents and reports describing forecast output. 1.5.1 Tools for literate programming Two effective and common tools for literate programming are: R Markdown — Allows code from multiple different languages including R, Python, SQL, C, and sh to be embedded within a common markup language (Markdown). Multiple different languages can be embedded within different blocks in the same document. Documents can be exported to a wide range of formats, including PDF, HTML, and DOCX. By default, R Markdown documents are static (i.e. the entire document is rendered all at once with a command); however, recent versions of RStudio allow them to be used interactively by rendering specific code blocks directly in the code editor window. R Markdown documents compiled to HTML format can easily embed interactive elements ranging from clickable plots and subsettable tables (e.g. htmlwidgets) to full applications with user-defined inputs (via RShiny); for more information, stay tuned for our follow up task view on Visualization. Jupyter — Unlike R Markdown, these were designed from the start to be used interactively. Documents are stored in a format that makes them difficult to edit with a plain-text editor; rather, they are typically edited using a special browser-based editor that runs a language “kernel” in the background. The results of any particular code block are stored across sessions, so code blocks do not need to be re-evaluated when exporting to other formats. A document can only use a single language, with Julia, Python, and R supported. 1.6 Workflows and Dependency Management Workflows are typically high-level descriptions of sets of tasks to be performed as part of an overall scientific application, at least in the context of this blog. There are a wide variety of methods and formats for expressing such descriptions. Workflows must include information about the tasks themselves, as well as their inputs and outputs, which either implicitly define or explicitly state dependencies among the tasks. This information, including the dependencies, is used by a Workflow Management System (WMS) to execute the tasks, potentially 1) on a local computer or one or more remote computers, including clouds and HPC or HTC systems; 2) serially or in parallel; 3) from the start or from a previous partially complete state. These dependencies can be static (fully defined before the application is run) or dynamic (e.g. partially defined based on data, execution, or other resources). Workflow management systems help efficiently reproduce portions of or entire scientific workflows. These tools analyze workflows, skip phases of the workflow that are up-to-date (if the exact inputs and tasks have been run previously, the previous outputs can be returned; this technique is sometimes called memoization), and execute tasks that are out-of-date, tasks downstream of out-of-date tasks, or tasks required to execute based on scheduled run times (e.g., daily-updating forecast). These tools are especially useful for large projects that bring multiple streams of data together in an analysis since it relieves the analyst from duties of keeping track of workflow order and tasks that need to be rerun. For example, when new data about a model parameter is included in the forecasting workflow, only the portion of the workflow dependent on that new data will be executed. Example of a simple dependency graph and which tasks will be executed using a workflow management system (from targets workflow example). 1.6.1 Tools for workflows and dependency management Below we list a few tools for workflows and dependency management. There are however many other workflow and dependency management tools. A larger list can be found here. Targets is an R-based ‘make’ like toolkit that tracks dependencies among phases of your workflow and executes work that is out-of-date. Targets builds upon previous R dependency managers such as drake and remake, and can deal with high-performance or -throughput computing (HPC/HTC) within the WMS framework. This includes automated detection and retries for model failures, and launching Slurm (or other job schedulers for HTC) jobs directly from a targets workflow. Additionally, there are built-in cloud integration capabilities to help scale larger compute projects. Video tutorial and other Drake resources Snakemake is a Python-based workflow management tool that includes a lot of the same functionality as Drake for R, including being compatible with HPC / HTC or cloud computing environments. The rules defined in a Snakemake target can use shell or Python commands or run external Python or R scripts, as well as utilize various remote storage environments such as Amazon S3, Dropbox, or Google Storage. Parsl is a Python library that lets users define a workflow through a Python program, or parallelize a Python program. They do this by ‘decorating’ the definition of Python functions and calls to external applications to indicate that they are potentially parallelizable and asynchronous tasks. When such a task is called, Parsl intercepts it and adds it to an internal dynamic directed acyclic graph that captures the overall application dependencies. If both the inputs for the task and execution resources are available, the task is run, and if not, it waits until these conditions are satisfied. In either case, Parsl immediately returns a ‘future’, a placeholder for the eventual return value, so that the overall application can proceed, which allows multiple tasks to run in parallel. Parsl is an open source project led by U Chicago &amp; Illinois, which supports a wide variety of execution resources (e.g., local, CPUs, GPUs, HPC, HTC, cloud) and schedulers. Pegasus is another scientific workflow system with a long history of development and use in the science world (e.g., it’s the workflow system used by LIGO) Argo is a more recent kubernetes-based workflow system, convenient when much of the workflow is within docker already (see Containerization below). Airflow is another workflow system, developed and used by AirBnB and others, mostly in industry. Airflow is now a project within the Apache Software Foundation. It allows a user to author workflows as Directed Acyclic Graphs (DAGs) of tasks. The Airflow scheduler executes the tasks on an array of workers while following the specified dependencies. It also has a user interface to allow the user to visualize pipelines running in production, monitor progress, and troubleshoot issues. 1.7 Unit Testing Ecological forecasting workflows can be complex and involve many steps from data ingest, data cleaning, producing forecasts, to visualizing output. Often, these forecasting workflows need to produce output on a regular schedule and ensuring that each part of the workflow performs appropriately is crucial for making forecasts and identifying failure points, whether operational or not. Unit testing is automated tests on small units within a larger workflow to ensure that the different sections behave as intended (e.g. testing that individual functions return the expected outputs for valid inputs and the expected errors for invalid inputs). Frequently unit tests are also used for regression testing, where a test is created for a previous bug or problem that is fixed. The regression test is used to prevent a bug from being reintroduced. In combination with continuous integration (see below), these tests ensure that modifications to a code base run as expected after the modifications have been integrated. In case of complex workflows or systems, a unit test will only test to make sure each of the components are working as intended. Additionally an integration or system test will need to be performed at certain points to test all the components interacting with each other. For example does each component still produce the outputs expected by the next steps in the workflow. 1.7.1 Tools for unit testing Most programming languages have a testing framework that will help with the unit tests. A list of tools here, some of the commonly used testing frameworks for tools used in forecasting are: testthat for R, including examples of how to implement unit testing in R. pytest for Python 1.8 Continuous Integration and Automation Both the models we use to make predictions and the forecasting workflows we build around them are, in some sense, always a work in progress. Any time we make changes to our models and workflows, whether it’s updating a library or adding a data source, there’s a chance that we’ll break our workflow. Tools for continuous integration enable researchers to update their forecasts and run tests on their code in an automated and robust manner (e.g. with system tests in place to check for accidental deployments that would otherwise break a deployment). Continuous Integration (CI) tools automatically builds and deploys software ecosystems, and tests new versions of code to ensure development of models will work. This is especially important for iterative forecasts that need to be deployed at regular intervals (e.g. daily forecasts). As CI tools continue to become more powerful, flexible, and generous with their service offerings, they can expand from supporting development workflows to even be used as the primary platforms for application workflows, such as iterative, real-time forecasting. Below we list few of these tools and a larger list can be found here or here: 1.8.1 Tools for continuous integration and automation Travis CI, Probably the most popular automated testing tool on GitHub, at least in the recent past. This service is designed to test builds and run unit tests (and other, short-lived scripts) on a variety of different virtual platforms with different configurations. Travic CI runs for free on its Travis CI servers, but has time and CPU limits (at least for the free version (though a user can request that these limits be increased). Some features include the ability to run actions in parallel (configured via a YAML file) and an ability to be accessed via an API. GitHub Actions, similar to Travis CI, but hosted natively by GitHub and with more generous time, memory, and CPU allowances for open-source (public) projects on GitHub. GitHub Actions is quickly increasing in popularity. GitLab CI, similar to Travis and GitHub Actions but hosted by GitLab. Circle CI, similar to Travis and GitHub Actions. Jenkins, a locally run alternative that you can deploy on your own servers. 1.9 Containerization Complex scientific workflows often involve combining multiple different tools written in different programming languages and/or possessing different software dependencies. Even a simple R script may depend on multiple R packages, and may only work as expected if specific versions of those packages are used. Managing these different tools and their dependencies can be a complex task, especially when tools conflict with each other (e.g. one tool may only work with an older version of a library, while another tool may only work with a newer version of the same library). As the number of tools and their dependencies in a workflow grows, managing these dependencies becomes challenging, and reproducing this workflow on a different machine (potentially with a different operating system) is even more challenging. Containers resolve these issues by providing a way to create isolated packages for each software element and its dependencies. These containers can then run on any computing environment (as long as it has the requisite container software itself installed). Moreover, containerization software sometimes allows for the creation of container stacks (a.k.a “orchestration”)— collections of multiple containers that communicate with each other (including sharing data) and with the host system in precise, user-defined ways (see Workflow and Dependency Management above). In some cases, these container stacks can be deployed across multiple physical or virtual computers, which greatly facilitates the process of scaling computationally intensive analyses. 1.9.1 Tools for containerization By far the most common tool for containerization — indeed, the emerging standard across the software development industry — is Docker. Docker containers are typically created from a definition file, basically just a starting container (e.g. a specific version of a Linux operating system) followed by a list of shell commands describing the installation and configuration of the specified software and its dependencies. Thousands of existing containers (any of which can be used as a starting point for a custom container) for a wide range of software are available on Docker Hub, a publicly available registry. Software stacks and workflows using multiple containers can be created via Docker Compose, which automatically configures and runs multiple interrelated Docker containers from a human-readable (YAML) specification file. Several tools for orchestration of Docker containers exist — Docker Swarm is distributed as part of Docker (i.e. no additional installation) and allows for rapid deployment with minimal configuration, while Kubernetes is a much more complex but feature-rich solution. Another quickly maturing tool leveraging Docker is The Binder Project, which is a relatively easy to use tool that turns a Git repository into a Docker image for deploying a reproducible computing environment in the cloud. Unfortunately, Docker’s design precludes its use on high-performance computing clusters and other enterprise-managed machines often encountered in the sciences. In particular, running Docker containers requires running a persistent background process with administrative (“root”) privileges on the host machine. This is not an issue on self-managed, isolated physical (e.g. your personal laptop) and virtual (e.g. Amazon Web Services) machines. However, it does pose a major security concern that precludes its use on high-performance computing clusters and other enterprise-managed machines often encountered in the sciences. Singularity is an alternative that was designed specifically to address these concerns. Unlike Docker, Singularity does not require a persistent background process to run — rather, its design involves creating containers that are fully self-contained executable files. These files can then be distributed just like any other files, and executed on any machine (as long as that machine has a compatible version of Singularity installed). The initial install of Singularity, as well as the creation of containers, does require root permissions, but unlike Docker, the containers themselves run as a single process with only user permissions. Besides the security implications, this design also makes Singularity containers more amenable to HPC queue submission systems (running the containers is effectively the same as running any other executable). Like Docker, Singularity containers can be created via a definition file, and can be stored on a free, publicly available registry (Singularity Hub). The major downside of Singularity is that it has a much smaller user base (largely limited to a small subset of the scientific community, compared to Docker’s widespread use in both science and industry), and is much less mature software. For example, while Singularity does provide a “Compose” interface, as of this writing this is still in early development and highly experimental. Singularity also works with Kubernetes. 1.10 Metadata Metadata provide crucial information on the ecological forecasting data, including model input, output, and parameters, among others. Metadata tells the user how to interpret model output and what conditions are needed to reproduce output. Metadata is also used to describe the size and dimensions of the dataset, quality of the data, author of the data, keywords of the project used to produce the data, and details on how the data were produced. Appropriately documenting ecological forecasting output helps other researchers find relevant datasets and reuse output for other applications such as input to other models, or cross-model comparison such as a forecasting challenge. 1.10.1 Tools for metadata Ecological Metadata Language (EML) is a community-maintained project for documenting research data with a readable XML markup syntax. EML serves the needs of the research community and is modularly designed to enable growth in the language as the needs of the earth and environmental sciences evolve. The Ecological Forecasting Initiative has developed additional forecast-specific standards using EML as the base metadata standards. The EML R package facilitates generating an EML document, however, these documents can also be created using a text editor or other scripting languages such as Python. EFI is in the process of drafting an ecological forecasting metadata standard that extends EML. Current info is located in our forecast-standards repo and in the pre-prints list on the EFI Publications webpage. Many other metadata standards can be found here. 1.11 Data and Code Release A core principle of creating reproducible scientific workflows is making the code and data used in the analyses available to the public through data and code publication or releases. It is now often required by journals or institutions to publish the data used in scientific publications and to a lesser extent, the code used in the analyses. Many of the other reproducible principles described above enable efficient data and code release and publication. For example, remote version control repositories, such as GitHub, display developmental and stable code bases and can tag versions of code to be released along with details on what the version was used for (e.g. “v1.2.1 used in analyses described by Dasari et al. 2019”). These code releases can also become citable with digital object identifier (DOI) by connecting with other archiving tools. Data releases should also be relatively painless if the previous principles of reproducible workflows are followed. Key to data releases and publishing in repositories are descriptive metadata that describe important characteristics of the dataset that is to be published (see Metadata section above). Additionally, embedding data publishing tasks (e.g. metadata descriptions, pushing data to a remote repository) in a dependency management system (see above) can make updating data in a public repository as easy as executing one line of code. 1.11.1 Tools for data and code release Zenodo – a general purpose open-access repository, often used with GitHub to publish software. DataOne – a repository for environmental and ecological data. Dryad – a general purpose repository for research data. Environmental Data Initiative – a repository for environmental data. ScienceBase – an open-access data repository maintained by the US Geological Survey. Open Science Framework (OSF) – an open-access repository for research data. Software Heritage Archive – a repository of open-access software. Registry of Research Repositories – a collection of information about &gt; 2000 research data repositories. "],["uncertainty-quantification-data-assimilation-modeling-statistics.html", "2 Uncertainty quantification, Data assimilation, Modeling &amp; Statistics 2.1 Overview 2.2 Empirical models 2.3 Mechanistic models 2.4 Uncertainty 2.5 Data assimilation", " 2 Uncertainty quantification, Data assimilation, Modeling &amp; Statistics 2.1 Overview Curators: Abby Lewis1, Ben Toh2, Jake Zwart3, Alexey Shiklomanov4, Leah Johnson1, Ethan White2, Hassan Moustahfid5, Kelly Heilman6, Ash Griffin7, Jody Peters8, Quinn Thomas1, Mike Dietze9 1Virginia Tech, 2University of Florida, 3USGS, 4NASA, 5NOAA, 6University of Arizona, 7MailChimp, 8University of Notre Dame, 9Boston University The crux of a successful forecasting system is an effective model with properly specified uncertainty. Numerous techniques are available to create a model for a given forecasting problem, and each modeling technique will require different mechanisms for incorporating, quantifying, and propagating uncertainty. Here, we outline the tools available for both empirical (statistical, Bayesian, and machine learning) and mechanistic models (Figure 1: C). To expand on existing modeling resources, we then describe how uncertainty can be incorporated into a forecasting workflow using different types of model and tools for assimilating new data to update a forecast (Figure 1: D). Figure 1. Conceptual diagram of the components in the forecast workflow, including the iterative forecast and adaptive management cycles. A (Data Ingest, Cleaning, Management) and D (Visualization/Decision Support Tools, User Interface) will be described in future task views. B (Data Assimilation) and C (Model) are described below. 2.2 Empirical models In general, empirical models let data speak for itself, making inferences and predictions without extensively encoding the underlying ecological process. These models are often fast and computationally efficient when making predictions. However, they are only informed by previous values in the dataset and therefore may be unlikely to perform well outside of the range of observed conditions. Furthermore, improper variable selection and assumptions on distributions can cause inaccurate predictions. In this section, we outline tools for two categories of empirical models: statistical models and machine learning. 2.2.1 Statistical model (frequentist) Statistical models use historical data to estimate statistical parameters, then use those estimates to forecast into the future. In this section we focus on some of the more commonly used tools for fitting statistical models and making forecasts within a frequentist framework. Time series models, e.g. Auto Regressive Integrated Moving Average (ARIMA) models, are a common choice of statistical model relevant to forecasting. These models focus on learning the temporal pattern of the past, including seasonality and temporal autocorrelation, and project this pattern into the future in a forecasting setting. 2.2.1.1 Tools for frequentist models The following are commonly used tools for fitting and running frequentist models for the interpreted programming languages R and Python. See the Reproducible Forecasting Workflows post for more details on these two interpreted languages. R The stats (aka “base R”) package provides time series functions such as ts() and arima() for representing time series objects and fitting ARIMA models. The forecast package offers more useful tools for fitting ARIMA models. For example, auto.arima() finds the best set of parameters for both seasonal and non-seasonal components of the ARIMA model based on Akaike information criterion (AIC) or Bayesian information criterion (BIC); forecast() makes predictions and great forecasting plots (with prediction uncertainty!) based on your choice of ARIMA model. It also provides useful statistical tests and cross-validation functions. See this online textbook. The nlme, lme4 and glmmTMB packages can be used to fit Generalized Linear Mixed Models (GLMM), incorporating autoregressive model (AR1) structure alongside other random effects. The mgcv and gamm4 packages provide the gamm() function to fit Generalized Additive Mixed Models, incorporating AR1 structure, random effects and splines, and spatial smoothers. Python The statsmodels module provides the core functions for working with ARIMA models including arima.model.ARIMA() for fitting models of specific orders and functions for comparing the fits of models with different orders. The pmdarima module provides a high level wrapper to statsmodels with equivalent functionality to ‘forecast::auto.arima()’ in R. The auto_arima() function provides automated model selection to determine the best seasonal and non-seasonal ARIMA models based on a suite of information criteria. This package also provides useful statistical tests and cross-validation functions. 2.2.2 Statistical model (Bayesian) In a Bayesian framework, all fitted parameters have a probability distribution. This is useful as it enables robust accounting of parameter uncertainty, and allows for the model to be informed by existing data or expertise. Often, a Bayesian framework also allows the user to fit more complicated and hierarchical time series models, while accounting for uncertainty in parameters, drivers, and observation data. Options for fitting Bayesian models include working directly with Markov chain Monte Carlo (MCMC) samplers, interfacing with existing MCMC or Gibbs samplers such as Just Another Gibbs Sampler (JAGS), Bayesian inference Using Gibbs Sampling (BUGS), or Stan, using readily available “native” functions, and Laplace approximation. These methods are all described in more detail below. 2.2.2.1 Work directly with Markov chain Monte Carlo (MCMC) R The mcmc package facilitates sampling from a posterior distribution using the Metropolis algorithm and provides other useful helper functions The MCMCpack package also provides a function to sample from a distribution using the Metropolis algorithm and provides other useful helper functions The BayesianTools package provides functions to run a range of different MCMC algorithms as well as non-MCMC sampling algorithms such as rejection sampling and sequential Monte Carlo (SMC). Also provides model selections and multi-model inference functionality. Treats the model as a ‘black box’ so particularly handy for calibrating mechanistic models. Python The pymc3 module allows models to be written using the Python language, and fits the model using various sampling algorithms. 2.2.2.2 Interface with JAGS/BUGS/Stan Instead of working directly with MCMC samplers, a model can be specified in the JAGS (Just Another Gibbs Sampler), BUGS (Bayesian Inference Using Gibbs Sampling), nimble, or Stan syntax. Stan can be faster for complex, hierarchical models without conjugacy. For simple models, ones that have conjugate relationships, or models with a lot of latent variables, JAGS/BUGS/NIMBLE are usually faster.These languages select samplers based on the model, and provide posterior samples. Differences in these programs are primarily in the backend algorithms. R rjags, R2jags and jagsUI are some of the packages that pass data and model specification to JAGS. The nimble package compiles and runs models in C, making them more computationally efficient. It provides a lot of flexibility to optimize samplers and create customized functions and samplers. See some examples of custom distributions for ecology. The rstan package interfaces R with Stan. bayesforecast package uses Stan as a backend to implement ARIMA, Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) and other forecasting models. Python The pyjags and pystan modules are used to interface with JAGS and Stan. 2.2.2.3 Readily available “native” functions By trading flexibility and customizability for some convenience, some packages allow us to fit Bayesian models with “native” functions. R brms lets us fit the ARMA model with syntax and formula familiar to every R user: e.g., fit &lt;- brm(y ~ x + arma(p = 1, q = 1), data = data). Uses Stan under the hood. rstanarm documentations provide a number of examples of fitting GLMM using native R syntax with Stan under the hood. glmmfields fits spatiotemporal GLMM using Stan under the hood. MCMCPack provides functions to fit numerous regression models under Bayesian framework. spBayes fits spatiotemporal GLMM using native R language The BMS package enables Bayesian model averaging, sampling data according to different g-priors and model priors and can work with a wide variety of samples 2.2.2.4 Laplace Approximation The Bayesian approach is generally much more time consuming than frequentist and machine learning. Fitting time for MCMC is slow and due to the iterative nature of MCMC, parallelization of individual chains is not possible. One of the most common and well supported ways is to approximate the posterior distributions using Laplace approximation. The R-INLA package (inla) provides ways to fit a wide variety of statistical models via the integrated nested Laplace approximation approach. It is now heavily used in temporal, spatial and spatiotemporal GLMM. 2.2.3 Spatio-temporal statistical models The book Spatio-Temporal Statistics with R by Wikle et al. (2019) provides an accessible introduction to both frequentist and Bayesian approaches to spatiotemporal modeling (i.e. models for forecasting across both space and time) using a range of packages in R. The book particularly emphasizes the use of basis functions to approximate spatiotemporal covariance structures. The website provides open text and hands-on activities. 2.2.4 Machine learning Unlike statistical models, Machine Learning (ML) models make few assumptions about probability distributions, instead relying on algorithms to learn patterns by themselves. ML allows for complex interactions among predictors (commonly called features in the ML community) without a priori specification. However, often it is best to carefully select the features used to train the ML models based on which features will most likely be influential to the variable being predicted. This feature selection can either be guided by ML models themselves, or by domain experts. Furthermore, outputs from other mechanistic or statistical methods can be used as features to train a ML model. Given enough data, ML methods often provide more accurate predictions than parametric statistical methods, largely due to their flexibility and limited a priori assumptions on variable distributions. Injecting process knowledge into ML techniques is an active and growing area of research (e.g. ‘theory-guided machine learning’, neural hierarchical models, process-guided deep learning). Methods that are generally considered ML include decision tree based methods (e.g. Classification And Regression Tree (CART, random forest, gradient boosted trees), support vector machines (SVM), and artificial neural networks (ANN). More complicated deep learning models are a form of ANNs and are increasingly utilized in ecological forecasting. Empirical Dynamic Modeling is a time-series specific machine learning approach that is often used in ecological forecasting. 2.2.4.1 Tools used for ML models R gbm and xgboost for Gradient boosted trees randomForest for random forest caret streamlines the process of fitting ML (and some stats) models, providing functions to pre-process data, conduct feature selections and parameter tuning, which is a very important aspect of ML. bartMachine to fit Bayesian Additive Regression Trees (BART), which infuse Bayesian framework with decision tree methods to achieve uncertainty quantification rEDM for Empirical Dynamic Modeling (based on cppEDM C++ library) Python scikit-learn (also known as sklearn) is the widely used ML library pyEDM is used for Empirical Dynamic Modeling (based on cppEDM C++ library) 2.2.4.2 Interface with machine learning platform/libraries For neural networks and deep learning, (e.g. the Long Short-Term Memory (LSTM) recurrent neural networks, which are popular in fitting time series), it is extremely common and popular to use a number of libraries that are based on or interface primarily with Python, e.g., Tensorflow, PyTorch and keras. Thanks to Rstudio, there are now packages that interface R with these libraries (tensorflow, keras and torch). 2.3 Mechanistic models Mechanistic models can range from simple deterministic (finite-difference or differential equation-based) models to highly complex “black box” simulators. Simple deterministic models are often designed using custom code, but tools are available for parameter fitting and manipulating differential equations. Fewer tools are available for model fitting using external executables or agent based models. 2.3.1 Simple deterministic model The simplicity of these is both a strength and weakness in a forecasting framework; simplifying a model structure reduces parameter uncertainty and may avoid overfitting, but it may obscure important ecological processes. Deterministic models are typically designed using custom code, and there are very few off-the-shelf tools to help create deterministic models. That being said, a few packages (listed below) are useful to help with parameter fitting and manipulating differential equations. For more information on (frequentist) parameter fitting in R, see this tutorial from R-bloggers. Bayesian inference for a limited variety of ordinary differential equations (ODEs) are available in the beta version of BUGS, or in R through the deBInfer package. Packages to note: Packages to fit ODEs deSolve in R ODEINT and GEKKO in Python DifferentialEquations.jl in Julia Packages to “run” a compiled model R – system2 function in base R; processx package Python – subprocess.call (part of the standard library) Packages for state space models (e.g. deterministic/stochastic latent process and statistical observation process) LibBi: C++ based library for state-space modelling and Bayesian inference supporting multiple cores. Estimate model likelihood and parameters using Sequential Monte Carlo (SMC), Particle MCMC (PMCMC), Kalman filter and others. Allows writing deterministic models in relatively simplistic script. RBi in R: R wrapper package to interface with LibBi pomp in R: statistical inference for partially-observed Markov processes (i.e., non-linear stochastic dynamical systems). Supports parameter estimations such as Particle MCMC (PMCMC), trajectory matching, improved iterated filtering (IF2), Approximate Bayesian Computation (ABC) and variations of Kalman Filter. SimBIID in R: R package mainly for simulation-based inference for infectious disease models. Provides simplistic syntax to write SIR models. Support ABC-SMC and PMCMC. nimble and nimbleSMC packages in R: Supports particle filtering and PMCMC. User creates a state-space model in BUGS. pyEMU is a Python module that interfaces with Pest++ to fit model parameters and estimate model uncertainty. PEST++ is model-independent and should be able to fit parameters given the correctly formatted inputs, which is facilitated by pyEMU 2.3.2 Black box models The previously-mentioned BayesianTools R package was originally designed to calibrate “black box” ecological models and provides an in-depth vignette for coupling such models to R. While optimized for terrestrial ecosystem models, the Predictive Ecosystem Analyzer (PEcAn) is a predominantly R-based workflow that includes utilities for efficient Bayesian calibration of black box models by using Gaussian Process models to emulate the Likelihood surface. More recently, this approach has been extended to a hierarchical across-site calibration. 2.4 Uncertainty Understanding how much uncertainty is present in ecological forecasts is essential to both scientific inference and decision making. Decisions based on a highly confident forecast will be very different from those based on a forecast with a wide range of possible outcomes, and an incomplete accounting of uncertainty will lead to falsely overconfident (and thus risk prone) decisions. Uncertainty accounting requires both quantifying uncertainty for models, or components of models, and propagating that uncertainty through other aspects of the full forecast. Forecasts may include a variety of different types of uncertainty such as parameter uncertainty, random effect uncertainty, initial condition uncertainty, covariate or driver uncertainty, and process uncertainty. 2.4.1 Statistical model (frequentist) For most frequentist models, uncertainty sources are limited to parameter uncertainty and residual error, which are produced by most of the tools described for statistical modeling above. Parameter uncertainty can also be estimated using bootstrapping and other similar methods. Tools for producing prediction intervals (the range of values expected to capture a percentage of future observations) for these models include R: forecast() function from the forecast package can be used to produce prediction intervals for many statistical models. Python: the same functionality is available in the get_forecast() function in statsmodels. 2.4.2 Statistical model (Bayesian) Statistical models that are formulated as Bayesian models have considerable flexibility in how multiple sources of uncertainty are incorporated and modeled. Standard inclusions are parameter uncertainty, residual process error, and observational uncertainty. Hierarchical models (the Bayesian versions of mixed effects models) allow variation in parameters between groups, and uncertainty in the parameters that describe the group level variation. Additionally, it is possible to explicitly include model uncertainty when using Bayesian methods, for example through Bayesian model averaging approaches. Typically estimates of all types of error/uncertainties, including predictive, must be obtained via Monte Carlo methods (as described below) as closed form solutions are only rarely available. 2.4.3 Machine learning models Machine learning covers a wide range of models with equivalently wide ranges of approaches to uncertainty, including some methods that lack uncertainty estimates entirely using their standard implementations. Since machine learning is often focused on prediction, many methods produce estimates of uncertainty in the predictions, which is useful in a forecasting context. Some approaches are implemented so that the tools for prediction intervals described in Statistical models (frequentist) can also be used with these models. For neural networks, Monte Carlo dropout and Gaussian mixture methods have also been used. Other approaches may require handling uncertainty in a manner specific to the modeling approach. 2.4.4 Mechanistic Models: Monte Carlo propagation and partitioning Most mechanistic models do not inherently include analytical uncertainty estimators, and thus uncertainty is usually incorporated using Monte Carlo methods (which can also be applied to any of the previous approaches). Monte Carlo methods involve running the model repeatedly with stochastic variation in either model inputs (e.g., incorporating uncertainty in the initial conditions and/or drivers), parameter values (to capture uncertainty in the parameters of the model), and/or residual/process error distribution. This is typically implemented using an ensemble approach, where each ensemble member has parameter and driver inputs drawn from a specified distribution (see Table 1 for tools). For models that include a temporal component uncertainty propagates into the future due to compounding differences in parameters and drivers between ensemble members. Uncertainty partitioning can be done using either global variance-based methods (e.g. Sobol indices) or one at a time (OAT) methods. Using OAT methods, all but one source of uncertainty is set to not have any variability, the contribution of that source of uncertainty to variability in the forecast output is determined, and this process is repeated for all other sources of uncertainty. A primer on Monte Carlo propagation and OAT partitioning is available as part of the Ecological Forecasting book. 2.4.5 Uncertainty in covariates One of the unique challenges related to uncertainty for forecasting is incorporating uncertainty in the value of future covariates. For example, a model that relies on climate covariates should include uncertainty in future climate conditions in forecasts. In Bayesian approaches this uncertainty can be incorporated directly into the model to make predictions. In other approaches it can be incorporated by running the model repeatedly using ensembles of covariates based on uncertainty in the covariate forecast. The different sets of predictions can then be incorporated using ensemble approaches (see Mechanistic Models: Monte Carlo propagation and partitioning). 2.4.6 Propagating uncertainty There are currently not many “off the shelf” tools for propagating uncertainty in forecasts, as many forecasting practitioners develop their own pipelines for analyzing and propagating uncertainty into their forecasts. However, a few tools do exist: R The spup package provides tools for spatial uncertainty propagation and analysis The sensitivity package provides tools for global variance-based sensitivity analyses that can be used for uncertainty partitioning Python Uncertainpy provides tools for uncertainty quantification and sensitivity analysis using quasi-Monte Carlo methods and polynomial chaos expansions Julia Measurements.jl package for propagating uncertainty using linear error propagation theory Additionally, many resources and tutorials exist that can be useful for both analyzing and propagating uncertainty: Uncertainty Analysis: Uncertainty Analysis YouTube tutorial (EFI/NEON series) UQWorld: uncertainty quantification community and resources Uncertainty Propagation YouTube Tutorials (EFI/NEON series): Tradeoffs &amp; Analytical Moments Linear Tangent Monte Carlo 2.5 Data assimilation Updating model predictions to incorporate new data is a central component of near-term ecological forecasting (Figure 1). One way of doing this would be to repeat the entire model-fitting procedure above any time new observations are added to a dataset. However, that may be very computationally expensive, especially for large complex models. Instead, one might prefer to update existing model predictions using just the new observations (and their uncertainties) and then re-generate new predictions starting from the updated model state. We describe this process as data assimilation. A Note on Vocabulary The term “Data assimilation” (or sometimes, “model-data fusion”) is often used to describe a variety of modeling activities. Some people use “batch data assimilation” or “parameter data assimilation” to describe fitting models to data (see earlier sections). Others use “data assimilation” to broadly refer to any activity that combines information from data and models in any way, such as initializing a model with observed conditions or using observations as model drivers or boundary conditions. For the purposes of this document, we use “data assimilation” to mean the specific approach we describe above; namely, iteratively updating model states using observations (sometimes called “state” data assimilation or “sequential” data assimilation; Figure 2). Figure 2. In a sequential data assimilation framework, estimates of model states (and optionally parameters) are updated as new observations are assimilated into the model. The updated states take into account state observations and modeled state estimates as well as the relative confidence in each (shown as distributions). The updated states are then used as initial conditions for the model to make predictions at the next time step (t+1) using the model’s equations. Figure from Ellen Bechtel and Jake Zwart. For a given time step, a data assimilation algorithm takes two things as inputs: a joint probability distribution of model predictions for all model variables, and distributions of observations of a subset of those model variables. The data assimilation algorithm then synthesizes this information from both the model and the data and produces as output a new joint posterior distribution of all model variables (taking into account covariance between model variables). This new joint posterior distribution is used as the new model initial condition to generate updated predictions into the future (Figure 2). 2.5.1 Data assimilation approaches Different data assimilation approaches (Table 1) differ in their assumptions about the distributions of the model and data predictions. Below we highlight some of the most commonly used data assimilation approaches. The simplest approach is the Kalman Filter (KF; EFI/NEON tutorial), which assumes that both model predictions and data have Gaussian (normal) distributions and that the model is linear. This assumption gives the Kalman filter a simple (iterative) analytical solution. Because of its computational efficiency and conceptual simplicity, implementations of the Kalman filter abound. If you are using a model ensemble to estimate model uncertainty, the ensemble Kalman filter (EnKF) first fits a multivariate normal distribution (i.e., calculates the sample mean and covariance of the ensemble) and then proceeds as the normal Kalman filter. For highly computationally demanding models, where one is limited in the size of ensemble that can be used, the unscented Kalman Filter (uKF) uses the complex “unscented transform” to sample ensemble members systematically (rather than the random sampling in EnKF) and analytically back-transform the ensemble predictions to estimate the mean vector and covariance matrix (rather than the simple sample mean and covariance in EnKF). The linear model assumption of the Kalman filter can be relaxed by solving for the linear tangent approximation of model, much as one would do in initial two terms of a Taylor Series; this class of methods is called the extended Kalman filter (eKF). Similar adjoint approaches are employed in variational data assimilation approaches (e.g. 4DVar). The most general, completely distribution-agnostic data assimilation approach is the particle filter (EFI/NEON tutorial link), which simply resamples your model ensemble members weighted according to their likelihood relative to the observations. This approach is conceptually simple and typically easy to implement, but requires very large model ensembles and a variety of resampling methods to avoid rapid loss of effective sample size (prediction collapsing onto a small number of ensemble members that do not represent the true spread of the predictive distribution). Data assimilation is an active area of research, particularly in atmospheric science (not least because of its importance to numerical weather prediction), and new data assimilation approaches are constantly released. Some exciting recent developments (at least to the authors) include LaVEnDAR (Pinnington et al. 2020, the Tobit-Wishart ensemble filter (TWEnF, Raiho et al. 2020, which estimates the process error dynamically and accounts for zero-bound and zero-inflated data), and strongly coupled Earth System data assimilation techniques (Penny et al. 2019), among others (Table 1). In theory, data assimilation effectively and elegantly links uncertainties in parameters (see previous sections) and states. In practice, using data assimilation to track uncertainties in parameters and states simultaneously can be challenging (e.g., avoiding rapid convergence of all ensemble members to a single point) and often requires careful algorithm tuning. For example, Kalman Filter implementations sometimes artificially inflate the variance of the distributions to avoid convergence (e.g., filter inflation), but figuring out the right amount of inflation typically requires a lot of problem-specific trial and error (or methods that solve for the process error dynamically, e.g. TWEnF). 2.5.2 Tools for data assimilation Numerous tools exist for data assimilation , though a majority of these tools are targeted to large-scale models and big data and may be more complicated than needed for smaller ecological forecasting applications. Table 1 provides a sampling of the data assimilation tools available, but is by no means a comprehensive list. Table 1: Examples of data assimilation tools Software Software programming language DA methods supported Target application and model programming language supported DART Fortran KF, enKF Large-scale models and big data in any language; originally created for atmospheric &amp; oceanic models Nimble R (with C++ backend) Particle Filter, enKF Small-scale models that can be implemented in R pomp R Particle Filter, enKF Small-scale models that can be implemented in R PyDA Python exKF, enKF, 3DVar, 4DVar Small-scale models implemented in Python; originally created for researchers with little DA experience KalmanFilter.jl Julia KF Small-scale models that can be implemented in Julia StateSpace.jl Julia KF, eKF, uKF, enKF Small-scale models that can be implemented in Julia ParticleFilters.jl Julia Particle Filter Small-scale models that can be implemented in Julia EMPIRE Python and Fortran Particle Filter, enKF, 3DVar, 4DEnVar Medium- to large-scale models that can be implemented in any language LaVEnDAR Python 4DEnVar Land surface models implemented in any language BayesianTools R Particle Filter Small- to Medium-scale models in any language OpenDA C/C++, Java, Fortran enKF, Steady State KF, Particle Filter, 3DVar, DudEnKF1 Small- to large-scale models that can be implemented in any language 1DudEnKF - Doesn’t Use Derivatives Ensemble Kalman Filter. See details here. "],["visualization.html", "3 Visualization 3.1 Overview 3.2 Guiding Principles 3.3 How We Make Visuals 3.4 Tables 3.5 References", " 3 Visualization 3.1 Overview Curators: Libby Mohr1, Matthew Brousil2, Kelly Heilman3, Hfassan Moustahfid4, Leah Johnson5, David LeBauer3, Rob Kooper6, Cee Nell7, Josh Cullen8, Jake Zwart7, Jody Peters9, Quinn Thomas5, Mike Dietze10 1Montana State University, 2Washington State University, 3University of Arizona, 4NOAA, 5Virginia Tech, 6National Center for Supercomputing Applications, 7USGS, 8Florida State University, 9University of Notre Dame, 10Boston University As the old adage goes “A picture is worth a thousand words.” At the end of any data gathering endeavor, one is faced with the task of dissemination. This step determines how the end-user consumes the information buried within the data. Often these data sets are complex and require creative strategies to communicate information accurately and effectively. Science, policy and planning rely on reliable and unbiased communications. Data visualizations and graphics are a powerful means to communicate. They have great impact on how we perceive environmental problems, their solutions, and if we consider policies legitimate. Over the last two decades, more and more studies have demonstrated that visualization plays a role in data-communication, influences decision making, public perception, public participation, and knowledge cocreation (Metze 2020). A famous example of a great data visualization is Charles Joseph Minard’s diagram of Napoleon’s march to Moscow, which clearly shows the disastrous number of lives lost in this military campaign. Such an image readily conveys a message “at a glance” that sticks to memory and is difficult to forget. Another example of influential visualizations is the diagram of the ‘Burning Embers’ from the IPCC report in 2001 (updated in 2009; Figure 1). This diagram visualizes the risks coming from the heating up of the earth. Studies show how these embers have been adapted and contested over time (Wardekker &amp; Lorenz 2019). Burning Embers Figure 1 from Smith et al 2009, shared with permission from lead author Joel Smith. Data visualization has a long history in scientific communication. Simple forms include pie charts, line graphs and more complex network diagrams. Newer techniques use larger datasets and allow users to interact with data to create their own visualizations “on demand”. For example, the large dataset that pertains to COVID-19 and its impact on various populations across the globe clearly demonstrates the impact of data visualization on science on policy (Johns Hopkins COVID-19 Dashboard). Data visualization requires an appreciation for how to convey relationships, categories, and magnitude through the creative use of lines, colors, symbols, position and size. Data visuals help tell a story, starting with a compelling problem or question, using data to provide perspective, and giving the reader some insight or solution. Yet creators must respect the limits in datasets. For example, maps of climate change show bands of color with projected changes in temperature or precipitation, but they are limited by the scarcity of weather observation stations across the developing world (see figure SPM.1 in the Summary for Policymakers of the IPCC’s Special Report on Global Warming of 1.5℃). To inform policy, data visualization is beginning to enrich our understanding of the challenges facing society. Evidence-based illustrations, or infographics, help convey complex policy issues and the potential trade-offs in greater depth than long reports or other media (Mclnerny et al. 2014). Finally, when subject matter is intangible (e.g., due to scale, complexity, or abstraction), visualizations have a fundamental role in exploring information and generating understanding. In addition to an open scientific infrastructure, visualization and graphics should be among the main priorities for developing modern science and science policy. The following Visualiztion Task View first provides guiding principles for creating visualizations and applications with static or interactive features, then provides examples and tools to create static, animated, interactive, and spatial visuals as well as resources for visualizing uncertainty. 3.2 Guiding Principles 3.2.1 Overview Project design, co-development, and sustainability, are all important aspects to consider when creating visualizations. The following principles can apply to a range of visualizations and applications ranging from scientific publications to user interface and decision support applications. Although often unseen by users, how one generates the information and visualizations used to convey that information is just as important as the science behind the information being shared. Here is an EFI Bibliography Zotero Library with books and papers on visualization best practices. In Zotero, use the Filter Tags section in the lower left-hand corner to select “EFI Visualization Paper” to bring up the references. Additional resources, specifically for web applications for visualizing scientific data can be found in this scientificwebapps Zotero Bibliography. 3.2.2 Design When developing visualizations, determine the intended purpose, use case, and motivation for the resources being developed. Will you be using the resource to conduct exploratory data analysis, to be used in a publication, or to allow an audience that is internal or external to the project to interact with the output? Think about who wants the information, what information is important, how important is that information, and what outcome or takeaways do you want the audience to grasp. Your target audience and intended outcomes will shape the development process and the design of the visualization tool. One way to brainstorm visualizations is to think about user stories, a concept from computer science. User stories are informal, general explanations of a software feature (but apply to visualizations as well) written from the perspective of the end-user. See Max Rehkopf’s 2022 user story template and examples to further consider how a visualization, app, or tool provides value to the end-user. Table 1 (below) provides suggestions and considerations when working with end-users, thinking about the life expectancy and sustainability of a visualization or application, using interfaces, communicating with end-users through visualization or application, and what kind of maintenance is needed. Table 1 comes from the sciwebapps project that is creating an open source and community-driven repository for resources, information, discussions, and suggested practices for creating, communicating, and improving Scientific Web Applications. In this 10-minute YouTube video, EFI Steering Committee member, Melissa Kenney provides three examples of how considering the design of visuals leads to “Improving Decision-maker Usability of Forecast Data Products.” 3.2.3 Co-development Depending on the type of visualization product you are creating and your audience, you may be collaborating with other users or parties on project development. If so, it is important to consider the co-development process. Co-development or co-production of a visualization or application takes place when Information Users and Information Providers collaborate to generate scientific resources. Ideally, the co-development process aims to build and sustain partnerships, acknowledging differences in what each party needs from the partnership and the project, and what the partners’ strengths and limitations are regarding their contributions. Co-production can take different forms depending on the project or funding ranging from projects with limited to no community participation (e.g., Contractual) to community members having authority over the research process (e.g., Collaborative, Collegial, Indigenous) (David-Chavez and Gavin, 2018). Additional information about co-production definitions, papers, info sheets, and frameworks and assessment tools that was shared by three panelists from a seminar on co-production hosted by the EFI Partners &amp; Knowledge Transfer Working Group in May 2021 can be found here. 3.2.4 Product Sustainability Best practices include identifying the intended lifespan of the product and the available resources (e.g. time, funding, computing resources) for maintaining and hosting the product. An important consideration for public-facing tools is how much traffic they can handle and what resources are needed to maintain or increase their scale or user base. 3.3 How We Make Visuals 3.3.1 Digital Accessibility (a11y) Digital accessibility, also referred to as ‘a11y’, is used in Web development to enable as many people as possible to use Web sites no matter an individual’s physical and cognitive abilities. See Burnett et al. 2021 for a brief discussion of a11y with respect to scientific web applications. Table 2 (below) provides a list of a11y resources with general information, standards and evaluation, assessment tools, and communities of practice that are useful to explore when creating web applications or other user interfaces. 3.3.1.1 Color Vision Evaluation and Accessibility Tools Tools for evaluating graphics with color vision simulators: Color Oracle standalone free color blindness simulator for Windows, Mac and Linux that shows in real time what people with common color vision impairments will see. Colorblindly Chrome extension web browser tool that simulates colorblindness on a web browser. Tools for creating colorblind friendly graphics include: Colorspace R and Python package that provides a toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in various kinds of visualizations. Color Brewer provides color options for making maps in R with a range of data classes in multiple color schemes and with colorblind, printer and photocopy safe options Viridis is an R package that provide a series of color maps that are designed to improve graph readability for readers with common forms of color blindness and/or color vision deficiency Tutorial 3.3.1.2 Alternative Text Alternative text are written descriptions of images and videos that can be read out loud so that the visual is not required. Captions or transcripts may be necessary for video animations with sound. See Writing Alt Text for Data Visualization for suggestions on how to write alt text for different types of data visualization, how to add alt text in a number of applications, and links to other resources 3.3.1.3 Motion Sensitivity To accommodate motion sensitivity, the WCAG advises: “For any moving, blinking or scrolling information that (1) starts automatically, (2) lasts more than five seconds, and (3) is presented in parallel with other content, there is a mechanism for the user to pause, stop, or hide it unless the movement, blinking, or scrolling is part of an activity where it is essential.” Video animations are preferred over GIFs for more complex animations because it gives playback control to users, it does not autoplay or loop infinitely, and it can be easily opted out of. Avoiding flashes, bouncing, and slower animations with short durations helps make motion more accessible. 3.3.1.4 Accessibility Considerations for Interactive Visualizations Some interactive features can be difficult to navigate for people navigating with a keyboard or using magnification software. Avoid making important information only available on hover Display pop-up content upon both hovering with a pointer and focusing with a keyboard More tips about pop-up content displayed by mouse hovers and keyboard focus here 3.3.2 Static Visuals Static visuals are those that don’t change with time or user-input. Such plots are useful for exploratory data analysis (e.g. deciding if and how to include variables in a forecast model), visualizing forecasts and associated uncertainty (see the Uncertainty Visualization Section), and assessing model performance. Whereas animated and interactive graphics allow for additional complexity and exploration, static visuals are well-suited for clearly conveying simple messages. In this section, we briefly describe common types of static plots used in forecasting. For each plot type, we list the tools that can be used to make them. We focus on tools in R, specifically those within base R and the ggplot2 ecosystem. We also provide examples of each static plot created using data from the EFI RCN NEON Ecological Forecasting Challenge. Click here to see the code for the R packages and NEON data used to create the plots below. knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE) # Show code and plots # Load packages library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ── ## ✔ ggplot2 3.3.6 ✔ purrr 0.3.4 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.4.1 ## ✔ readr 2.1.2 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union library(here) ## here() starts at /Users/runner/work/taskviews/taskviews library(arrow) ## ## Attaching package: &#39;arrow&#39; ## ## The following object is masked from &#39;package:lubridate&#39;: ## ## duration ## ## The following object is masked from &#39;package:utils&#39;: ## ## timestamp library(cowplot) ## ## Attaching package: &#39;cowplot&#39; ## ## The following object is masked from &#39;package:lubridate&#39;: ## ## stamp library(patchwork) ## ## Attaching package: &#39;patchwork&#39; ## ## The following object is masked from &#39;package:cowplot&#39;: ## ## align_plots library(ggtext) library(colorspace) library(viridis) ## Loading required package: viridisLite library(ggridges) library(GGally) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 library(hexbin) library(ggdist) ## ## Attaching package: &#39;ggdist&#39; ## ## The following objects are masked from &#39;package:ggridges&#39;: ## ## scale_point_color_continuous, scale_point_color_discrete, ## scale_point_colour_continuous, scale_point_colour_discrete, ## scale_point_fill_continuous, scale_point_fill_discrete, ## scale_point_size_continuous library(ggExtra) # Get target data aquatics_targets &lt;- readr::read_csv(&quot;https://data.ecoforecast.org/targets/aquatics/aquatics-targets.csv.gz&quot;) ## Rows: 10491 Columns: 10 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): siteID ## dbl (8): oxygen, temperature, chla, oxygen_sd, temperature_sd, chla_sd, dep... ## date (1): time ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. terrestrial_daily_targets &lt;- readr::read_csv(&quot;https://data.ecoforecast.org/targets/terrestrial_daily/terrestrial_daily-targets.csv.gz&quot;, guess_max = 1e6) ## Rows: 19980 Columns: 4 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): siteID ## dbl (2): nee, le ## date (1): time ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. pheno_targets &lt;- readr::read_csv(&quot;https://data.ecoforecast.org/targets/phenology/phenology-targets.csv.gz&quot;) ## Rows: 42804 Columns: 6 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): siteID ## dbl (4): gcc_90, rcc_90, gcc_sd, rcc_sd ## date (1): time ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. tick_targets &lt;- readr::read_csv(&quot;https://data.ecoforecast.org/targets/ticks/ticks-targets.csv.gz&quot;) ## Rows: 528 Columns: 4 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): siteID ## dbl (2): mmwrWeek, amblyomma_americanum ## date (1): time ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. beetle_targets &lt;- readr::read_csv(&quot;https://data.ecoforecast.org/targets/beetles/beetles-targets.csv.gz&quot;, guess_max = 1e6) ## Rows: 3234 Columns: 4 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): siteID ## dbl (2): abundance, richness ## date (1): time ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. For visualizing data in Python, we recommend checking out the commonly-used Matplotlib, Pandas, and Seaborn libraries. In Julia, the Plots.jl metapackage provides a common interface to several plotting Backends and the StatsPlots package provides additional functionality needed to produce many of the plots described below. Spreadsheet platforms (e.g. Microsoft Excel and Google sheets) also provide interactive tools for making simple static plots. Additional references for R, Python, and Julia are provided at the end of this section. 3.3.2.1 Distributions When developing forecasts it is often useful to examine the distribution of a single variable or parameter. Such visualizations can be used to examine 1) how data are distributed and 2) uncertainty in parameter estimates or other estimated quantities. There are a few different “flavors” of distribution plots, depending on the application. 3.3.2.1.1 Histograms Histograms are generated by breaking the data up into several bins and then counting the number of data points that fall into each bin. hist - function in base R geom_histogram - in ggplot2 geom_bar() + scale_x_binned() - in ggplot2 stat_histinterval - from the ggdist package geom_bin_2d - in ggplot2 - creates a 2D histogram showing joint distribution of two variables by binning observations into squares in a Cartesian plane geom_hex - in ggplot2 - creates a 2D histogram showing joint distribution of two variables by binning observations into hexagons in a Cartesian plane ggMarginal - drop-in function ggMarginal adds marginal histograms to existing ggplots. Click here to see an example of geom_bar code and plot. terrestrial_daily_targets %&gt;% drop_na(nee) %&gt;% filter(siteID == &quot;KONZ&quot;, month(time) == 2) %&gt;% ggplot(aes(x = nee)) + geom_bar(width = 0.9, fill = &quot;#588B8E&quot;)+ scale_x_binned()+ theme_minimal_hgrid()+ xlab(&quot;net ecosystem exchange (g C m&lt;sup&gt;-2&lt;/sup&gt; day&lt;sup&gt;-1&lt;/sup&gt;)&quot;)+ ylab(&quot;number of days&quot;)+ theme(axis.title.x = element_markdown()) Figure 3.1: Histogram of daily net ecosystem exchange of carbon dioxide (NEE) in February at the Konza Prairie Biological Station NEON site, 2019-2022 Click here to see an example of geom_hex and ggMarginal code and plot. p &lt;- ggplot(pheno_targets, aes(x = gcc_90, y = rcc_90)) + geom_hex(bins = 30)+ geom_point(color = &quot;transparent&quot;)+ ylab(&quot;greenness index&quot;) + xlab(&quot;redness index&quot;)+ theme_cowplot()+ scale_fill_continuous_sequential(palette = &quot;BurgYl&quot;)+ theme(legend.position = c(0.8, 0.8)) ggMarginal(p,type = &quot;histogram&quot;, fill = &quot;grey80&quot;, color = &quot;white&quot;, bins = 30) Figure 3.2: Hexagonal heatmap showing the joint distribution of greenness chromatic coordinate and redness chromatic coordinate at the six NEON sites in the phenology forecast challenge. 3.3.2.1.2 Density Plot A Density Plot is a smoothed, continuous version of a histogram that displays the estimated probability distribution of a variable or parameter. Density plots are often used to visualize Bayesian prior and posterior distributions. When data or MCMC samples are involved, probability density is typically estimated using the “kernel density estimation” method, where the smoothness and shape of the density curve are controlled by the choice of bandwidth parameter and kernel, respectively (see Wilke, 2019: Visualizing distributions: Histograms and density plots). geom_density - in ggplot2 - plots density estimates for a single variable geom_density_2d - in ggplot2 - plots contours resulting from a 2D kernel density estimation geom_function - in ggplot2 - plots a function (e.g. dnorm) geom_area - in ggplot2 - can be used for a filled density plot slab and halfeye- This vignette describes the slab+interval geoms and stats in ggdist. Click here to see an example of geom_area code and plot. ggplot() + geom_area(data = tibble(x = seq(0.01, 4, 0.01), y = dlnorm(seq(0.01, 4, 0.01))), aes(x = x, y = y), fill = &quot;grey80&quot;, color = &quot;grey40&quot;)+ xlim(c(0,4))+ theme_minimal_hgrid()+ ylab(&quot;probability density&quot;)+ theme(axis.title.x = element_blank()) Figure 3.3: Density plot for a log-normal distribution with mean = 0 and standard deviation = 1 3.3.2.1.3 Dotplots Dotplots show the individual observations of a distribution, either as continuous values or as part of binned intervals (similar to histograms). This type of visualization provides the finest scale of visualization for distributions that may be lost when using other common methods (i.e., histograms, density plots, boxplots). Quantile Dotplots are similar, except that the data are broken into quantiles, which are then binned and plotted such that each dot represents a single quantile rather than a single observation. Quantile dotplots are a way of visualizing distributions and uncertainty with a “frequency frame”, and have been shown to aid people in making decisions in the face of uncertainty (Kay et al., 2016). geom_jitter in ggplot2 - adds small amounts of variation to the location of each point to avoid overplotting when visualizing observations of a quantitative variable. geom_dotplot in ggplot2 - bins up observations and plots a single point for each observation. stat_dots from the ggdist package - plots a single point for each binned observation by default, or produces a quantile dotplot if the quantiles argument is specified. Click here to see an example of stat_dots code and plot. set.seed(123) tempSample &lt;- tibble(temp = rnorm(10000, mean = 17.7, sd = 1)) ggplot(tempSample, aes(x = temp, fill = stat(x &gt; 19), color = stat(x &gt; 19))) + stat_dots(quantiles = 30)+ geom_vline(xintercept = 19, linetype = 2, color = &quot;gray50&quot;)+ theme_cowplot() + xlab(&quot;Temperature (°C)&quot;)+ scale_fill_manual(values = c(&quot;#517AC9&quot;, &quot;#C05D5D&quot;))+ scale_color_manual(values = c(&quot;#517AC9&quot;, &quot;#C05D5D&quot;))+ theme(legend.position = &#39;none&#39;, axis.title.y = element_blank(), axis.text.y = element_blank(), axis.line.y = element_blank(), axis.ticks.y = element_blank()) Figure 3.4: Quantile dotplot showing the probability of temperature exceeding 19 °C for a theoretical, normally-distributed temperature forecast with mean = 17.7 °C and sd = 1 3.3.2.1.4 Boxplots Boxplots display common summary statistics for a distribution, which include the minimum, first quartile, median, third quartile, and maximum values. The ‘box’ displays the middle 50% of the distribution while the ‘whiskers’ display the remaining 50% in the tails of the distribution. Any values that fall outside of 1.5 times the length of the ‘box’ are typically drawn as points beyond the whiskers and are typically referred to as outliers. Similarly, interval plots typically show the median as a point and one or more lines or boxes whose widths represent quantiles of the distribution. Whereas the plots described above are useful for visualizing single distributions, boxplots, interval plots, violin plots, and ridgeline plots can display multiple distributions in the same plot (Wilke, 2019). boxplot in base R geom_boxplot in ggplot2 interval and pointinterval from the ggdist package create stand-alone interval plots, whereas several other stats from the stat_slabinterval family allow one to add intervals to other plot types, including density plots, violin plots, histograms, and dotplots. Click here to see an example of geom_boxplot code and plot. terrestrial_daily_targets %&gt;% drop_na(le) %&gt;% filter(month(time) == 5, siteID %in% c(&quot;BART&quot;, &quot;CLBJ&quot;, &quot;KONZ&quot;, &quot;ORNL&quot;, &quot;OSBS&quot;)) %&gt;% ggplot(aes(x = siteID, y = le))+ geom_boxplot(fill = &quot;#78A481&quot;) + theme_minimal() + xlab(&quot;NEON site&quot;) + ylab(&quot;latent heat flux (W m&lt;sup&gt;-2&lt;/sup&gt;)&quot;)+ theme(axis.title.y = element_markdown()) Figure 3.5: Distributions of daily latent heat flux during May at 5 NEON sites shown as boxplots. 3.3.2.1.5 Violin Plots Violin plots are an extension of a density plot where the density plot is mirrored and shows the minimum and maximum values, as well as the maximum point density region(s). Since they can show greater detail of a distribution, violin plots have often been used as a replacement for boxplots more recently. geom_violin in ggplot2 pirateplot from the yarrr package stat_eye from the ggdist package Click here to see an example of geom_violin code and plot. terrestrial_daily_targets %&gt;% drop_na(le) %&gt;% filter(month(time) == 5, siteID %in% c(&quot;BART&quot;, &quot;CLBJ&quot;, &quot;KONZ&quot;, &quot;ORNL&quot;, &quot;OSBS&quot;)) %&gt;% ggplot(aes(x = siteID, y = le))+ geom_violin(fill = &quot;#78A481&quot;) + theme_minimal() + xlab(&quot;NEON site&quot;) + ylab(&quot;latent heat flux (W m&lt;sup&gt;-2&lt;/sup&gt;)&quot;)+ theme(axis.title.y = element_markdown()) Figure 3.6: Distributions of daily latent heat flux during May at 5 NEON sites shown violin plots. 3.3.2.1.6 Ridgeline Plots Ridgeline plots show offset density plots for multiple variables. This can be useful for making comparisons of a given variable over space or time, as well as for comparing among a number of groups. However, it is not possible to directly compare the values on the y axis among density plots due to the three dimensional effect of this type of plot. Therefore, this method may be more useful when interested in relative (rather than absolute) densities. geom_density_ridges from the ggridges package stat_halfeye from the ggdist package Click here to see an example of geom_density_ridges code and plot. terrestrial_daily_targets %&gt;% drop_na(le) %&gt;% filter(month(time) == 5, siteID %in% c(&quot;BART&quot;, &quot;CLBJ&quot;, &quot;KONZ&quot;, &quot;ORNL&quot;, &quot;OSBS&quot;)) %&gt;% ggplot(aes(x = le, y = siteID))+ geom_density_ridges(fill = &quot;#78A481&quot;) + theme_minimal() + ylab(&quot;NEON site&quot;) + xlab(&quot;latent heat flux (W m&lt;sup&gt;-2&lt;/sup&gt;)&quot;)+ theme(axis.title.x = element_markdown()) Figure 3.7: Distributions of daily latent heat flux during May at 10 NEON sites shown as staggered density plots or ‘ridgeline’ plots. 3.3.2.2 Point and line plots 3.3.2.2.1 Scatterplots Scatterplots are handy for visualizing relationships between two continuous variables. This includes time series, where the variable on the x-axis is time. The following tools are commonly used for making scatterplots: The plot function from the base package in R defaults to producing a scatter plot With ggplot2, the geometric object geom_point can be used Click here to see an example of geom_point code and plot. aquatics_targets %&gt;% filter(year(time) &lt;= 2020, year(time) &gt;= 2019, siteID == &quot;POSE&quot;) %&gt;% ggplot(aes(x = temperature, y = oxygen))+ geom_point(size = 1.2)+ ylim(c(7.5, 13.0))+ xlim(c(0,25))+ theme_minimal_grid()+ ylab(expression(paste(&quot;dissolved oxygen (mg &quot;, L^-1, &quot;)&quot;)))+ xlab(&quot;Temperature (°C)&quot;) Figure 3.8: Scatterplot of dissolved oxygen concentrations vs. temperature at the Posey Creek NEON site, 2019-2020 3.3.2.2.2 Line Graphs Line Graphs connect data points sequentially and are also commonly used for visualizing time series. It’s also possible to add lines to existing scatter plots to emphasize connections between the data while emphasizing the data themselves with points. The tools below are often used to create line graphs or add lines to existing plots: The plot function from the base package in R generates a line plot with the argument type = “l” The lines function in base R adds a line to an existing plot. With ggplot2, the geometric object geom_line (or geom_path) can be used to add lines. Click here to see two examples of geom_line, with and without geom_point code, and the plots. aquatics_targets %&gt;% filter(year(time) &lt;= 2020, year(time) &gt;= 2019, siteID == &quot;POSE&quot;) %&gt;% ggplot(aes(x = time, y = oxygen))+ geom_line()+ theme_minimal_grid()+ ylab(expression(paste(&quot;dissolved oxygen (mg &quot;, L^-1, &quot;)&quot;)))+ theme(axis.title.x = element_blank()) Figure 3.9: Line graph of mean daily dissolved oxygen concentrations at the Posey Creek NEON site, 2019-2020 aquatics_targets %&gt;% filter(year(time) == 2021, month(time) == 6, siteID == &quot;POSE&quot;) %&gt;% ggplot(aes(x = time, y = oxygen))+ geom_line()+ geom_point()+ theme_minimal_grid()+ ylab(expression(paste(&quot;dissolved oxygen (mg &quot;, L^-1, &quot;)&quot;)))+ theme(axis.title.x = element_blank()) Figure 3.10: Line graph with dots showing dissolved oxygen concentrations at the Posey Creek NEON site, June 2021 3.3.2.2.3 Pairs Plots and Correlograms Pairs Plots and Correlograms are useful for quickly visualizing relationships and correlations between several variables at once. These plots are typically used for exploratory data analysis. pairs function from base R - creates scatterplot for each combination of variables ggpairs from the GGally package - by default, displays the correlation between each combination of variables in the top right corner, a scatter plot for each combination of variables in the lower left corner, and a density plot for each variable on the diagonal. Several customization options are available. corPlot from the psych package - displays the correlation between each combination of variables in a box, where the color hue of the box indicates whether the correlation is positive or negative and the color value indicates the strength of the correlation. Click here to see an example of ggpairs code and plot. Sys.unsetenv(&quot;AWS_DEFAULT_REGION&quot;) Sys.unsetenv(&quot;AWS_S3_ENDPOINT&quot;) Sys.setenv(AWS_EC2_METADATA_DISABLED = &quot;TRUE&quot;) s3 &lt;- arrow::s3_bucket( &quot;drivers/noaa/neon/gefs&quot;, endpoint_override = &quot;js2.jetstream-cloud.org:8001&quot;, anonymous = TRUE) df &lt;- arrow::open_dataset(s3) fc &lt;- df %&gt;% filter(start_time == as.Date(&quot;2022-04-20&quot;), site_id == &quot;ORNL&quot;, variable %in% c(&quot;TMP&quot;, &quot;PRES&quot;, &quot;RH&quot;, &quot;DSWRF&quot;, &quot;DLWRF&quot;, &quot;UGRD&quot;, &quot;VGRD&quot;)) %&gt;% select(horizon, ensemble, variable, predicted) %&gt;% collect() %&gt;% pivot_wider( names_from = variable, values_from = predicted, values_fn = mean ) %&gt;% mutate(wind_speed = sqrt(UGRD^2 + VGRD^2)) %&gt;% rename( &quot;air temperature&quot; = TMP, &quot;air pressure&quot; = PRES, &quot;longwave flux&quot; = DLWRF, &quot;shortwave flux&quot; = DSWRF, &quot;relative humidity&quot; = RH, &quot;wind speed&quot; = wind_speed, ) fc[sample(nrow(fc), 300), ] %&gt;% select(&quot;air temperature&quot;, &quot;air pressure&quot;, &quot;relative humidity&quot;, &quot;wind speed&quot;, &quot;longwave flux&quot;) %&gt;% ggpairs()+ theme_minimal_grid(font_size = 10) Figure 3.11: Combined scatterplot matrix and correlation matrix for five weather variables from the noaa GEFS model at the Oak Ridge National Lab NEON site. 3.3.2.3 Barplots and Heatmaps Barplots and Heatmaps are both used to visualize and compare quantities among one or more grouping variables. 3.3.2.3.1 Barplots Barplots encode quantities using bar length, where each bar represents a group. Stacked and grouped barplots allow for comparison among two different categorical variables. barplot from base R geom_bar and geom_col from ggplot2 Click here to see an example of geom_col code and plot. tick_targets %&gt;% filter(year(time) == 2019, month(time) == 6, siteID %in% c(&quot;UKFS&quot;, &quot;TALL&quot;, &quot;SERC&quot;, &quot;ORNL&quot;, &quot;KONZ&quot;)) %&gt;% group_by(siteID) %&gt;% summarize(`amblyomma_americanum` = mean(`amblyomma_americanum`)) %&gt;% arrange(desc(`amblyomma_americanum`)) %&gt;% mutate(siteID = factor(siteID, levels = siteID)) %&gt;% ggplot(aes(x = siteID, y = `amblyomma_americanum`))+ geom_col(fill = &quot;#7C7BB2&quot; )+ ylab(&quot;*Amblyomma americanum* density (ticks per 1600 m&lt;sup&gt;2&lt;/sup&gt;)&quot;)+ xlab(&quot;NEON site&quot;) + theme_minimal_hgrid()+ theme(axis.title.y = element_markdown()) Figure 3.12: Barplot showing the mean density of amblyomma americanum larvae observed in June 2019 at 5 NEON sites 3.3.2.3.2 Heatmaps Heatmaps encode quantities using color, where the x and y position of each block in the heatmap correspond to the first and second categorical variables, respectively. heatmap from base R geom_tile from ggplot2 Click here to see an example of geom_tile code and plot. siteIDLevels &lt;- beetle_targets %&gt;% filter(year(time) &gt;= 2017, year(time) &lt;= 2019) %&gt;% group_by(siteID) %&gt;% summarise(meanRichness = mean(richness, na.rm = TRUE)) %&gt;% arrange(meanRichness) %&gt;% pull(siteID) expand_grid(year = 2017:2019, month = 1:12, siteID = unique(beetle_targets$siteID)) %&gt;% mutate(monthYear = paste0(year, &quot;-&quot;, if_else(nchar(month) == 1, paste0(&quot;0&quot;, month), as.character(month)))) %&gt;% left_join(beetle_targets %&gt;% mutate(month = month(time), year = year(time), monthYear = paste0(year, &quot;-&quot;, if_else(nchar(month) == 1, paste0(&quot;0&quot;, month), as.character(month)))) %&gt;% filter(year &gt;= 2017, year &lt;= 2019), by = c(&quot;siteID&quot;, &quot;monthYear&quot;)) %&gt;% mutate(siteID = factor(siteID, levels = siteIDLevels)) %&gt;% ggplot(aes(x = monthYear, y = siteID)) + geom_tile(width = 0.9, height = 0.9, aes(fill = richness)) + theme_cowplot(font_size = 12)+ scale_fill_viridis(option = &quot;B&quot;, na.value = &quot;#BBBBBB&quot;, direction = -1, end = 0.9) + ylab(&quot;site&quot;)+ theme(axis.line = element_blank(), axis.ticks = element_blank(), axis.title.x = element_blank(), axis.text.x = element_blank(), panel.background = element_rect(fill = &quot;white&quot;), axis.text.y = element_text(size = 9)) + coord_cartesian(clip = &#39;off&#39;) + annotate(&quot;text&quot;, x = c(6, 18, 30), y = c(-3.7, -3.7, -3.7), label = c(&quot;2017&quot;, &quot;2018&quot;, &quot;2019&quot;)) + annotate(&quot;segment&quot;, x = 1, xend = 12, y = -2.8, yend = -2.8) + annotate(&quot;segment&quot;, x = 13, xend = 24, y = -2.8, yend = -2.8) + annotate(&quot;segment&quot;, x = 25, xend = 37, y = -2.8, yend = -2.8) + annotate(&quot;text&quot;, x = 1:36, y = rep(-1, times = 36), label = rep(c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;March&quot;, &quot;April&quot;, &quot;May&quot;, &quot;June&quot;, &quot;July&quot;, &quot;Aug&quot;, &quot;Sept&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;), times = 3), angle = &#39;90&#39;, size = 3) Figure 3.13: Heatmap of monthly beetle species richness observations at 47 NEON sites, 2017-2019. 3.3.2.3.3 Composite plots Composite plots are created by combining multiple plots into the same graphic. facet_wrap and facet_grid in ggplot2 - breaks the data into groups based on one or more grouping variables and displays a plot for each group plot_grid from the cowplot package The patchwork package grid.arrange from the gridExtra package Click here to see an code combining two plots into the same graphic. filteredData &lt;- pheno_targets %&gt;% filter(siteID == &quot;HARV&quot;, year(time) == 2021) red &lt;- ggplot(filteredData, aes(x = time, y = rcc_90))+ geom_line(color = &quot;#841859&quot;) + ylab(&quot;redness index&quot;)+ theme_minimal_grid()+ theme(axis.title.x = element_blank()) green &lt;- ggplot(filteredData, aes(x = time, y = gcc_90))+ geom_line(color = &quot;#005600&quot;) + ylab(&quot;greenness index&quot;)+ theme_minimal_grid()+ theme(axis.title.x = element_blank()) red + green+ plot_layout(nrow = 2, byrow = FALSE) Figure 3.14: Stacked line plots of greenness and redness index at the Harvard Forest NEON site in 2021. 3.3.2.4 References R Fundamentals of Data Visualization From data to Viz | Find the graphic you need Welcome | ggplot2 R Graphics Cookbook, 2nd edition 3 Data visualisation | R for Data Science 28 Graphics for communication | R for Data Science Python Introduction to Data Visualization in Python | by Gilbert Tanner Chart Visualization — pandas 1.3.4 documentation Usage Guide — Matplotlib 3.4.3 documentation seaborn: statistical data visualization — seaborn 0.11.2 documentation Julia Plotting Backends StatsPlots 3.3.3 Animated Visuals Gif, video, and other animations are a powerful way to communicate complexity and patterns where static visualizations may fall short. Animations provide an additional dimension that can allow an audience to see progression in data over time or depth, track changes, or layer in additional information to support narration. This may be useful to avoid overwhelming a viewer and guide the audience’s attention to specific data points. Further, whereas static, 2D representations of 3D objects are difficult to interpret, animated representations where the object is rotated and displayed at different angles allow viewers to create a mental reconstruction of the 3D object. Finally, animation can be used to convey uncertainty using hypothetical outcome plots (HOPs), which are animated sequences of plots where each plot represents a random draw from an underlying distribution. Research suggests that HOPs lead to better understanding of uncertainty and more accurate judgments about differences between multiple random variables (Hullman et al., 2015; Kale et al., 2019). See more details about HOPs in the Uncertainty Visualization section. Click here to see an example of a hypothetical outcome plot. library(tidyverse) library(lubridate) library(cowplot) library(read4cast) library(gganimate) library(magick) library(av) # read EFI null temperature forecast for the aquatics challenge null_forecast &lt;- read4cast::read_forecast(paste0(&quot;https://data.ecoforecast.org/forecasts/aquatics/&quot;, &quot;aquatics-2020-09-01-EFInull.csv.gz&quot;)) %&gt;% mutate(time = as_date(time)) %&gt;% filter(variable == &quot;temperature&quot;, site_id == &quot;BARC&quot;) observed &lt;- neon4cast::combined_scores(&quot;aquatics&quot;) %&gt;% filter(time &gt;= lubridate::as_datetime(&#39;2020-09-01&#39;), time &lt; lubridate::as_datetime(&#39;2020-09-08&#39;), variable == &quot;temperature&quot;, !is.na(observed)) %&gt;% group_by(time) %&gt;% summarise(observed = median(observed)) set.seed(8953762) n_frames &lt;- 100 # Total number of ensemble members in animation random_ensembles &lt;- sample(unique(aquatics_null$ensemble), size = n_frames) HOP &lt;- null_forecast %&gt;% filter(ensemble %in% random_ensembles) %&gt;% mutate(ensemble_rank = dense_rank(ensemble)) %&gt;% uncount(n_frames, .id=&quot;frame&quot;) %&gt;% filter(ensemble_rank &lt;= frame) %&gt;% arrange(frame, ensemble_rank, time) %&gt;% group_by(frame) %&gt;% mutate(alpha = if_else(ensemble_rank == frame, 1, 0.5)) %&gt;% ungroup() %&gt;% mutate( datetime = as_datetime(paste(time, &quot;00:00:00&quot;)), xmin = datetime - hours(9), xmax = datetime + hours(9) ) %&gt;% ggplot() + geom_segment( aes(x = xmin, xend = xmax, y = predicted, yend = predicted, alpha = alpha, color = &#39;predicted&#39;), size = 2) + transition_manual(frame) + geom_point(data = observed, aes(x = time, y = observed, color = &#39;observed&#39;)) + scale_color_manual( name = NULL, values = c(&#39;observed&#39; = &quot;#000000&quot;, &#39;predicted&#39; = &#39;#00bfff&#39;), labels = c(&#39;observed&#39;, &#39;predicted&#39;), guide = guide_legend(override.aes = list(linetype = c(0,1), shape = c(16,NA)))) + scale_x_datetime(date_breaks = &quot;1 day&quot;, date_labels = &quot;%b-%d&quot;) + ylab(&quot;temperature (degrees Celsius)&quot;) + theme_minimal_hgrid() + scale_alpha(guide = &#39;none&#39;) + theme(axis.title.x = element_blank(), legend.position = &#39;right&#39;) + ggtitle(&#39;Number of ensemble members: {frame}&#39;) animation &lt;- animate(HOP, fps = 2.5) anim_save(&quot;./HOP.gif&quot;, animation = animation) gif &lt;- image_read(&quot;./HOP.gif&quot;) image_write_video(gif, path = &quot;./HOP.mp4&quot;, framerate = 2.5) Hypothetical outcome plot showing uncertainty in a 7-day water temperature forecast by sequentially adding each of 100 ensemble members. 3.3.3.1 Animation tools gganimate package Transitions define how animations are mapped to data transition_time transitions data through time, with the duration of each frame relative to the distance in time transition_reveal gradually reveals data transition_states animates between distinct stages or groups rayshader package Create movies of camera moving around 3D maps and ggplots with render_movie. Camera path around visualization can be custom defined. ImageMagick (CLI) / Magick (R) Manipulate images (rotate, scale, crop, trim, flip, blur, etc) and stitch into animation (image_animate in R). gifski package Sequence any series of frames into a gif using gifski Convert gifs to video ‘gif and animate’ macros in Julia gif creates an animated gif file. Use this for simple, one-off animations. animate returns and Animation object for later processing. Use this for anything complex or when you need full control of the life-cycle of the animation ImageIO - Python package 3.3.4 Interactive and Reactive Features Interactive data visualizations are particularly useful when you find yourself with more information than can realistically be included in a single visualization. Interactive features give control to the end-user, allowing them to access details associated with a particular plot feature or even customize the data being plotted. Common interactive features include those that allow users to zoom and pan over an image, hover or click on features to access tooltips with additional information, and hover over a single object to highlight the entire group to which it belongs. Reactive visualizations are a special type of interactive visualization where user input changes the information or data being displayed. Reactivity can allow users to do things like filter data to visualize subsets of interest, modify which variables are plotted, and customize how the data is plotted (e.g. histogram bin size, color palette). Here are some principles to consider when developing interactive research products. Scientists developing interactives should understand their users, uses, and usage (impact). Interactives should be developed and disseminated in the open whenever possible. Funders should develop and enforce standards for interactives as scholarly products. Organizations and communities of practice should enable discovery by improving findability. Organizations should reward researchers for creating useful interactives through existing incentivization structures. Publishers and funding agencies should create and enact publication standards. 3.3.4.1 Interactive tools Plotly (standard types of plots, 3D plots, etc) is a javascript visualization library with wrappers for R, Python, Julia, Matlab. ggplotly is a function within the plotly R package for making ggplot graphics interactive Resource for working with plotly in R - Interactive web-based data visualization with R, plotly, and shiny Highcharter is an R wrapper for the Highcharts javascript library and its modules. Highcharts is a flexible and customizable javascript charting library and it has a great and powerful API. Examples and details about highcharter can be found here Charter is an R wrapper for the Charts javascript library echarts4r is an R wrapper for the Apache eCharts javascript library ggiraph is an R package for making ggplot graphics interactive Dygraphs is an open source JavaScript charting library. It can handle large data sets (plots millions of points without getting bogged down), provides strong support for error bars and confidence intervals, and is highly customizable. The rgl and rayshader packages in R allow the creation of interactive 3D plots. 3.3.4.2 High-level Reactive tools High-level reactive tools provide figures or graphs that change as a function of user input. Tableau - is a subscription-based visualization tool that’s popular in the data viz community. Students and instructors may obtain free licenses to use it. Making visually compelling figures and plots is relatively easy with the powerful graphical user interface. Basically, you create a visualization using drag and drop. Dashboard and interactive visualization can be created and shared. With public data, one can share them via Tableau Public. COVID-19 in Florida is one example that uses the Tableau Public platform, which has been well utilized over the COVID-19 pandemic to disseminate information due to its ease in creating great visualizations. PowerBI - is in many cases similar to Tableau, featuring an easy to use graphical user interface that allows us to create visualizations. Power BI is well integrated with Microsoft, particularly suitable for people who’re already using Azure and Office in their workflow. While Power BI Desktop is free, a subscription-based Power BI Pro is required to publish and share the visualization created. Observable - is a Javascript based interactive notebook that allows you to create interactive visualizations based on reader’s input. Observable makes it easier to program in Javascript and to utilize the D3 library to make visually compelling plots. The notebook can be published and shared to the public. It is free for individuals, but collaboration beyond five persons requires paid subscriptions. Dash - is a framework for building and deploying data apps with customized user interfaces in Python, R, Julia, and F# (experimental). Shiny - is an R package that makes it easy to build interactive web apps straight from R. You can host standalone apps on a webpage or embed them in R Markdown documents or build dashboards. You can also extend your Shiny apps with CSS themes, htmlwidgets, and JavaScript actions. This webpage of Shiny resources provides example apps in Ecology, Conservation, and Epidemiology, links to Shiny and R programming resources, and statistical modeling, a Bayesian course, and Tidymodels EFI hosted a 4-part seminar series on R Shiny applications and the associated code needed for the applications. The applications included: Visualization of Data in Space and Time: An Interactive Framework Creative Visualization of Model Results and Uncertainty in Shiny Improving Speed of Shiny Apps by Pre-Computing Models A Primer to Creating Interactive Maps with Leaflet in Shiny Interact.jl - is a Julia package for creating web-based widgets See Table 3 (below) for a list of web-application development frameworks. 3.3.5 Geospatial Visuals Spatial datasets contain information that links the data to physical/geographic locations. Because spatial data has extra information, visualization, modeling, and manipulating spatial data requires special file formats, projections, and geospatial operations. We will briefly describe these here, but refer to existing resources on Geographic Information Systems (GIS) and working with static spatial data. Geocomputation with R is an online book on geographic data analysis, visualization and modeling 3.3.5.1 File Formats Files containing spatial data need to convey the spatial coordinates and how coordinates in geographic space are mapped onto a flat map surface. Thus, these data are often stored in different file formats than non-spatial data. Vector: A vector stores spatial data in a vector format (as opposed to gridded), such as in points, lines, and polygons geojson.io - json or text based standard that is commonly sent with APIs. geojson.io is a fast, simple tool to create, change, and publish maps using geojson data geopackage - based on SQLite (derived from spatialite and updated for OGC standard compliance) Shapefile - this is a widely adopted ArcGIS format; requires multiple files (shp, shx, dbf, etc) to be complete, often combined into a single zip Raster: Raster file formats store spatial data in pixels along a regular grid. There are a variety of different raster file formats, including GeoTIFF, ASCII, netcdf, cloud optimized geotiff, etc 3.3.5.2 Projections and Coordinate Reference Systems (CRS) Projections and coordinate reference systems both contain information about how spatial data can be visualized, and where it is, with important differences. Geographic Coordinate Reference Systems contain information on how spatial data is projected or placed on the Earth’s surface. ESPG codes are often used to refer to different coordinate reference systems Most spatial data formats (shapefiles, rasters, etc) store the geographic coordinate system with the data, so users only need to worry about this if they wish to switch the Geographic Coordinate Reference System to match that of another spatial dataset. Additional information about Geographic Coordinate Systems (where the data is located on earth) and Projected Coordinate Systems (how the data is drawn on a flat surface) can be found in this ArcGIS blog post. 3.3.5.3 Static Spatial Visualization Often, we need to visualize data and/or forecasts distributed in geographic space. Here we list some static spatial visualization tools in R and Python which can be helpful to open, work with, and visualize spatial data. These packages and tools may help when dealing with issues specific to geospatial data, for example, opening geospatial file types, making maps with appropriate spatial projections, plotting different spatial data types including gridded raster data and polygons and other shapefiles. 3.3.5.3.1 Geospatial operations &amp; tools for working with spatial data: Many of the software programs commonly used in ecological forecasting have capabilities to work with geospatial data. Here we provide links to these resources and packages ArcGIS - requires a purchased license to operate, although often academic, government, and industry organizations make ArcGIS available to students or employees QGIS - is free and open source R packages raster - contains useful functions for working with, manipulating, and visualizing rasters in R. The R community is moving towards deprecating this tool in favor of terra terra - is primarily a faster version of the raster package built by the same author but also handles some aspects of vector data. Currently terra is suggested for use over raster, but may have some compatibility issues with other packages. The raster package is still suggested when working with netCDF files that have multiple layers. maps - draws geographical maps sf - supports a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. sp - is now deprecated in favor of the sf package. This wiki page contains some equivalent commands between packages. tmap - generates thematic maps. The syntax for creating plots is similar to that of ggplot2, but tailored to maps. ncdf4 is a package for working with netCDF files in R stars - used to read, manipulate, write and plot spatiotemporal arrays with space and time array dimensions rgdal provides bindings to the ‘Geospatial’ Data Abstraction Library (‘GDAL’) (&gt;= 1.11.4) and access to the projection/transformation operations from the ‘PROJ’ library. Note that ‘rgdal’ will be retired by the end of 2023 so plan to transition to sf/stars/terra functions using ‘GDAL’ and ‘PROJ’ at your earliest convenience. Python Rasterio GeoPandas Pysal Xarray: - useful for working with netCDF files 3.3.5.4 Types of Plots for Spatial Data A choropleth map links polygons (e.g., counties, states, etc.) to values of a variable in a dataset using color or symbols. See an example here. Proportional or graduated symbol maps use symbols scaled either proportionally to values of a variable or based on classifications of a variable. See an example here. A dot map uses points to show the distribution of data spatially. Dots can be either at specific xy coordinates, or evenly distributed within an area of interest depending on the intent. See an example here. An isopleth map uses contours to connect identical values on a map and show the distribution of a variable spatially. See an example here. Cartograms are maps that scale polygons (e.g., counties, states, etc.) based on the values of a variable. Similar conceptually to a proportional symbol map, but the underlying map features themselves are scaled instead of a point layer on top. See an example here. Dasymetric maps type refine chloropleth maps using additional information to make the spatial distribution of mapped values more accurate. Reference for all the plot examples above: https://www.cdc.gov/dhdsp/maps/gisx/resources/thematic-maps.html 3.3.5.5 Interactive Spatial Visualization Many spatial datasets contain additional dimensions that can be represented in visualizations. For example, a dataset might have both spatial and temporal information, and you want to visualize how the geographic patterns in the data change over time, or visualize how different scenarios can change geographic distribution of your data. These tools provide some capability to create interactive spatial visualization: 3.3.5.5.1 Tools to create interactive maps:** ArcGIS Online - again requires a purchased license which may be available through your institution QGIS - has a QGIS2Web plugin. Here is a tutorial for the plugin. R Shiny - R package to create interactive web apps including maps Plotly - a graphing library in R that can be used to make maps geom_sf() for ggplot2 rasterVis package to plot rasters more effectively in R Leaflet is an open-source javaScript library for interactive maps. Leaflet can be used on its own or with Shiny. Mapbox is a mapping platform for building custom, interactive maps that can be embedded into web applications. Mapbox also provides a variety of vector and raster tilesets (e.g. roads, satellite imagery) and a geocoding API. Panoply is a stand alone netcdf visualizer for local or remote catalogs 3.3.6 Uncertainty Visualization Communicating forecast uncertainty can play a critical role in both aiding decision-makers and building and maintaining public trust. When depicting uncertainty, it is important to consider the audience, their numeracy, and their level of training in statistics. Below we briefly discuss several methods for visualizing uncertainty and present tools that can be used to implement each. Error bars are commonly used to depict uncertainty in the scientific community. When using error bars, it is important to clearly specify what the error bar represents, since error bars can be used to represent different measures of uncertainty (e.g. standard deviations, standard errors, confidence intervals, prediction intervals). Because interpreting many of these uncertainty measures requires a firm grasp of statistics, using error bars can lead to misinterpretation or inappropriate inference, especially by lay audiences (Hofman et al., 2020; Joslyn &amp; LeClerc, 2012; Franconeri et al., 2021). geom_errorbar in ggplot2 geom_ribbon in ggplot2 - analogous to an error bar but for lines rather than points point_interval from ggdist package Depicting a full distribution rather than summarizing it using an error bar can be an effective way to vizualize uncertainty. Compared to error bars, distribution visualizations provide the viewer with more information and have been linked to better decision making (Franconeri et al., 2021). See the Distributions section above for tools. Uncertainty can also be mapped to different visual channels, including color value or luminance, fuzziness, size, transparency, and location (MacEachren et al., 2012). These channels can be particularly useful when both x and y positions are already being used to represent another aspect of the data (e.g. on a map). However, these visual channels are less precise than encoding uncertainty with position, as in the case of error bars and distributions (Franconeri et al., 2021). In ggplot the aes() function can be used to map the fill, size, and alpha aesthetics to uncertainty. Uncertainty can be mapped to fuzziness using the with_blur() function from ggfx package Value-suppressing uncertainty palettes leverage the fact that color hues are easier to tell apart when they are saturated and darker (Correll et al., 2018). Such palettes map a variable of interest to color hue, and uncertainty to color saturation and luminance. The result is that values associated with high levels of uncertainty are more difficult to ascertain. Value-suppressing uncertainty palettes can be applied to visualizations like choropleth maps and heatmaps. multiscales package for ggplo2 is an experimental package that can help visualize uncertainty Another approach to visualizing uncertainty is to re-frame probabilities (e.g. 10%) as frequencies (e.g. 1 in 10). This can be an effective way to communicate uncertainty to a broader audience, including individuals with low numeracy. (Peters et al., 2010; Franconeri et al., 2021). Several plot types take advantage of this so-called “frequency-framing” approach, which allows viewers to infer probability from visual representations of frequency: Quantile dotplots depict distributions by representing individual quantiles of the data as dots. Quantile dotplots have been shown to aid people in making decisions in the face of uncertainty (Kay et al, 2016). stat_dots from the ggdist package produces a quantile dotplot if the quantiles argument is specified. Icon arrays depict ratios with a frequency-frame by depicting a large number of icons and coloring them according to one or more variables. riskyr package ggwaffle package Visualizing individual data points, ensemble members, or samples from a distribution can convey uncertainty more intuitively while providing a more nuanced representation of the data underlying a distribution.(Weissgerber et al 2015; Franconeri et al., 2021) geom_jitter in ggplot can be used to avoid overplotting when plotting several individual data points Hypothetical outcome plots (or HOPs) are animated sequences of plots where each plot represents a random draw from an underlying distribution. Research suggests that HOPs lead to better understanding of uncertainty and more accurate judgments about differences between multiple random variables (Hullman et al., 2015; Kale et al., 2019). See tools for Animated Visuals above. 3.3.6.1 Other Resources for Uncertainty Visualization Fundamentals of Data Visualization - section 16 specifically discusses visualizing uncertainty The Science of Visual Data Communication - an overview of approaches to uncertainty visualization ggdist package ungeviz package Recording of a panel hosted by the EFI Social Science Working group on visualizing uncertainty in forecasts including the use of color in NOAA weather maps 3.4 Tables 3.4.1 Table 1 - End-Users Table 1: Considerations for end-users, life expectancy and sustainability of the app, interface outputs, communicating to end-users and feedback, and app maintenance and updates. Consideration Description Recommendations Benefits Consequence of Ignoring End-users Who is my end-user? (Arguably the most important questions) Identify potential communities with which to communicate throughout the process. Design the interface for the anticipated end-users. Reach out to the anticipated end-users to identify needs, skills, etc. Make it easier to streamline the dissemination process. Creating a product that has no clear end-users End-users What are the needs of my end-users? Ask potential end-users. Improve chances of actually meeting the needs of end-users. Improve broader impacts of your research. Fail to meet the needs of the end-users. End-users What are the skills or prior knowledge of my end-users? Ask end-users or always assume that end-users have the minimal amount of skills or prior knowledge, but don’t “dumb it down”. Make your interface easier to use by stakeholders. Make the content more easily interpretable. Opportunity to create different options for end-users with varying skills, while keeping the barrier to entry low for users without certain skills. Isolating end-users without necessary skills or knowledge may lead to non-adoption. Life expectancy and sustainability How long will this application be relevant to end-users? Life expectancy and sustainability How will I ensure the application is available for that period of time? Life expectancy and sustainability What are the costs (time, money, bytes) associated with hosting or storing for this period of time? Life expectancy and sustainability Where will this application be located in 1 years time? 5? 10? Consider storing the source code for your application within a trusted digital repository. Consider using persistent identifiers for released versions. Be certain you will always have access to the host location (e.g., website), and if there are associated fees consider whether you anticipate to always pay the hosting fees. Releasing the initial version and each thereafter requires only a single location (URL, URI, DOI) to maintain and to share. Changing the location (e.g., URL) of an application will be to the detriment of the usage of your application. If you publish the first release of this software inside a research journal article, but change the location some months or years down the road, it will render the intial reference useless. Will also add another layer to the findability of your application. Interface outputs What is the purpose of the interface? Interface outputs How will information be exported from the interface? Communication How will I communicate to potential end-users? If you have identified the end-user types or individuals, then a communication plan becomes more constrained. Consider how effective communication through various outlets will be. Will a research article reach your intended audience? How about a Tweet? How about an email to certain individuals? Communication Will I provide an option for feedback on the interface? If so, how? The easier it is for the end-user to provide feedback, the higher the probability is that the end-user will supply it. Maintenance and updating What are the costs associated with maintaining the source code and user interface? 3.4.2 Table 2 - a11y Resources Table 2: a11y resources with general information, standards and evaluation, assessment tools, and communities of practice that are useful to explore when creating web applications or other user interfaces. Resources Type Consideration Website General information U.S. federal government-wide IT accessibility program. Includes recommendations, and standards for creating accessible digital products. Also provides many tools for evaluating digital products for accessibility. https://www.section508.gov/ General information Open access front-end checklist for website design by David Dias. The .io webpage includes a DIY checklist and report for evaluating websites. https://frontendchecklist.io/; https://github.com/thedaviddias/Front-End-Checklist#accessibility General information A11Y Project. A open-source resource and community of practice for advancing accessibility (A11Y) in the digital space. Provides digestable information, tips, and tools for creating accessible digital products. https://a11yproject.com/ General information Yale University’s curated list of tips, techniques, and resources for web accessibility. https://usability.yale.edu/web-accessibility/articles/links General information Tutorials, documentation, and resources for web accessibility curated and hosted by Mozilla. https://developer.mozilla.org/en-US/docs/Web/Accessibility Standards and evaluation A quick start guide/evaluation of a product from W3C https://www.w3.org/WAI/test-evaluate/preliminary/ Standards and evaluation The Worldwide Web Consortium’s (W3C) Web Content Accessibility Guidelines (WCAG) is an eminent standard (recommendations) for web accessibility. These guidelines provide self-evaluation criterion for meeting one of three standards in order of less to more accessible: A, AA, or AAA. https://www.levelaccess.com/wcag-2-1-in-amp/; https://www.w3.org/TR/WCAG21/ (guidelines v2.1) Assessment tool Web Accessibility Evaluation Tool (WAVE) for evaluating websites for accessibility. https://wave.webaim.org/ Assessment tool Functional Accessibility Evaluator (University of Illinois). An open source tool for evaluating websites for Level A and AA WCAG standards. https://fae.disability.illinois.edu/ Assessment tool Section 508 ICT manual baseline testing for accessibility https://github.com/Section508Coordinators/ICTTestingBaseline Community of Practice U.S. federal government accessibility community of practice https://www.cio.gov/about/members-and-leadership/accessibility-cop/ Community of Practice Worldwide Web Consortium (W3C) Web Accessibility Initiative (WAI) https://www.w3.org/WAI/ 3.4.3 Table 3 - Web-application Development Frameworks Table 3: List of web-application development frameworks. This table expands on the comes from the Supplement Information in Valle et al. 2019. + These tools are based on Graphical User Interfaces (GUI) and are stand-alone software. § These tools were not originally included in Valle 2019. Name computer language Websites Bokeh Python/Scala/Julia/R https://bokeh.pydata.org/; http://hafen.github.io/rbokeh/ Django Python https://www.djangoproject.com/ Flask Python http://flask.pocoo.org/ Vega/Vega-Lite Python (via Altair)/R (via reticulate) https://altair-viz.github.io/; https://github.com/vegawidget/altair Plotly/Dash R/Python/Matlab/Julia https://plot.ly Htmlwidgets R http://www.htmlwidgets.org/ Shiny/ShinyDashboard R https://shiny.rstudio.com/;https://rstudio.github.io/shinydashboard/ Leaflet R/Python (via folium) https://rstudio.github.io/leaflet/; https://github.com/python-visualization/folium Tableau None+ https://www.tableau.com RapidMiner None+ https://docs.rapidminer.com/7.6/server/how-to/create-web-apps/ PowerBI§ None+ https://powerbi.microsoft.com/en-us/what-is-power-bi/ Spotfire§ None+ https://www.tibco.com/products/tibco-spotfire Superset§ None+ https://superset.apache.org/ 3.5 References David-Chavez DM, Gavin MC (2018) A global assessment of Indigenous community engagement in climate research. Environ Res Lett 13:123005. https://doi.org/10.1088/1748-9326/aaf300 Franconeri et al. (2021) The Science of Visual Data Communication: What Works: Psychological Science in the Public Interest. https://doi.org/10.1177/15291006211051956 Hofman JM, Goldstein DG, Hullman J. (2020) How Visualizing Inferential Uncertainty Can Mislead Readers About Treatment Effects in Scientific Results. Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, pp. 1–12. doi:10.1145/3313831.3376454 Hullman J, Resnick P, Adar E (2015) Hypothetical Outcome Plots Outperform Error Bars and Violin Plots for Inferences about Reliability of Variable Ordering. PLOS ONE 10:e0142444. https://doi.org/10.1371/journal.pone.0142444 Joslyn SL, LeClerc JE. (2012) Uncertainty forecasts improve weather-related decisions and attenuate the effects of forecast error. Journal of Experimental Psychology: Applied, 18(1), 126–140. https://doi.org/10.1037/a0025185 Kale et al. (2019) Hypothetical Outcome Plots Help Untrained Observers Judge Trends in Ambiguous Data. IEEE Transactions on Visualization and Computer Graphics 25:892–902. https://doi.org/10.1109/TVCG.2018.2864909 MacEachren et al. (2012) Visual Semiotics &amp; Uncertainty Visualization: An Empirical Study. IEEE Transactions on Visualization and Computer Graphics, 18(12):2496–2505. https://doi.org/10.1109/TVCG.2012.279 McInerny, G J. et al. (2014) Information visualization in science and policy - engaging users &amp; communicating bias. Trends in Ecology &amp; Evolution, 29: 148-157. https://doi.org/10.1016/j.tree.2014.01.003 *Metze, T (2020). Visualization in environmental policy and planning: a systematic review and research agenda. Journal of Environmental Policy and Planning. https://doi.org/10.1080/1523908X.2020.1798751 Rehkopf, M (2022) User Stories with Examples and a Template. In: Atlassian Agile Coach. https://www.atlassian.com/agile/project-management/user-stories. Accessed 11 Apr 2022 Smith et al. (2009) Assessing dangerous climate change through an update of the Intergovernmental Panel on Climate Change (IPCC) “reasons for concern.” Proceedings of the National Academy of Sciences 106:4133–4137. https://doi.org/10.1073/pnas.0812355106 Valle D, Toh KB, Millar J (2019) Rapid prototyping of decision-support tools for conservation. Conservation Biology. 33:1448-1450. https://doi.org/10.1111/cobi.13305 Wardekker, A, Lorenz, S (2019). The visual framing of climate change impacts and adaptation in the IPCC assessment reports. Climatic Change, 156(1–2), 273–292. https://doi.org/10.1007/s10584-019-02522-6 Weissgerber et al. (2015) “Beyond bar and line graphs: time for a new data presentation paradigm.” PLoS biology 13.4: e1002128. https://doi.org/10.1371/journal.pbio.1002128 Wilke CO (2019) Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures, 1st edition. O’Reilly Media, Sebastopol, CA. https://clauswilke.com/dataviz/ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
